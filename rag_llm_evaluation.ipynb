{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG LLM Evaluation with Embedding Metrics\n",
        "\n",
        "This notebook evaluates RAG performance by:\n",
        "1. Taking a question and selected document IDs\n",
        "2. Creating prompts with retrieved documents\n",
        "3. Passing to an LLM (**Ollama - runs locally, no API limits!**)\n",
        "4. Comparing generated answers with ground truth using:\n",
        "   - **Text-based metrics** (exact match, F1 score)\n",
        "   - **Embedding-based metrics** (cosine similarity, euclidean distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import ollama\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 918 questions\n",
            "                                            question     answer  id\n",
            "0  Was Abraham Lincoln the sixteenth President of...        yes   0\n",
            "1  Did Lincoln sign the National Banking Act of 1...        yes   2\n",
            "2                   Did his mother die of pneumonia?         no   4\n",
            "3      How many long was Lincoln's formal education?  18 months   6\n",
            "4       When did Lincoln begin his political career?       1832   8\n"
          ]
        }
      ],
      "source": [
        "# Load Q&A data\n",
        "qa_df = pd.read_csv('data/rag-mini-wikipedia_q_and_a.csv')\n",
        "print(f\"Loaded {len(qa_df)} questions\")\n",
        "print(qa_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 3200 documents\n",
            "                                             passage  id\n",
            "0  Uruguay (official full name in  ; pron.  , Eas...   0\n",
            "1  It is bordered by Brazil to the north, by Arge...   1\n",
            "2  Montevideo was founded by the Spanish in the e...   2\n",
            "3  The economy is largely based in agriculture (m...   3\n",
            "4  According to Transparency International, Urugu...   4\n"
          ]
        }
      ],
      "source": [
        "# Load document data\n",
        "doc_df = pd.read_csv('data/rag-mini-wikipedia_document.csv')\n",
        "print(f\"Loaded {len(doc_df)} documents\")\n",
        "print(doc_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setup Ollama (Local LLM)\n",
        "\n",
        "**‚úÖ Ollama is already running on your system!**\n",
        "\n",
        "You have these models installed:\n",
        "- `llama3` (8B params) ‚Üê **Using this one**\n",
        "- `gemma3` (3B params)\n",
        "\n",
        "**To use a different model:**\n",
        "```bash\n",
        "# Change OLLAMA_MODEL in the cell below to one of:\n",
        "OLLAMA_MODEL = \"llama3\"   # Currently selected\n",
        "OLLAMA_MODEL = \"gemma3\"   # Alternative option\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Ollama is running!\n",
            "üì¶ Available models:\n",
            "   - ('models', [Model(model='llama3:latest', modified_at=datetime.datetime(2025, 10, 7, 13, 18, 0, 391725, tzinfo=TzInfo(-14400)), digest='365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1', size=4661224676, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0')), Model(model='gemma3:latest', modified_at=datetime.datetime(2025, 10, 7, 12, 9, 38, 308655, tzinfo=TzInfo(-14400)), digest='a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a', size=3338801804, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M'))])\n",
            "\n",
            "‚úì Using model: llama3\n"
          ]
        }
      ],
      "source": [
        "# Configure Ollama\n",
        "OLLAMA_MODEL = \"llama3\"  # Using your installed model\n",
        "\n",
        "# Test connection to Ollama\n",
        "try:\n",
        "    # List available models\n",
        "    models_response = ollama.list()\n",
        "    \n",
        "    # Parse the response - it might be a dict with 'models' key or have different structure\n",
        "    if isinstance(models_response, dict) and 'models' in models_response:\n",
        "        models_list = models_response['models']\n",
        "    else:\n",
        "        models_list = models_response\n",
        "    \n",
        "    # Extract model names - handle different response formats\n",
        "    available_models = []\n",
        "    for model in models_list:\n",
        "        if isinstance(model, dict):\n",
        "            # Try different possible keys\n",
        "            name = model.get('name') or model.get('model') or str(model)\n",
        "            available_models.append(name)\n",
        "        else:\n",
        "            available_models.append(str(model))\n",
        "    \n",
        "    print(f\"‚úì Ollama is running!\")\n",
        "    print(f\"üì¶ Available models:\")\n",
        "    for model in available_models:\n",
        "        print(f\"   - {model}\")\n",
        "    \n",
        "    # Check if selected model is available\n",
        "    if any(OLLAMA_MODEL in str(model_name) for model_name in available_models):\n",
        "        print(f\"\\n‚úì Using model: {OLLAMA_MODEL}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Model '{OLLAMA_MODEL}' not found.\")\n",
        "        print(f\"Available: {available_models}\")\n",
        "        print(f\"Run: ollama pull {OLLAMA_MODEL}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
        "    print(f\"\\nDebug info:\")\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nMake sure Ollama is running.\")\n",
        "    print(\"\\nTo start Ollama:\")\n",
        "    print(\"  1. Open the Ollama app from Applications, OR\")\n",
        "    print(\"  2. Run in terminal: ollama serve\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup Embedding Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model loaded: multi-qa-mpnet-base-dot-v1\n"
          ]
        }
      ],
      "source": [
        "# Initialize embedding model for semantic similarity evaluation\n",
        "# Using the SAME model as in Embedding_process.ipynb for consistency\n",
        "embedding_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
        "print(f\"Embedding model loaded: multi-qa-mpnet-base-dot-v1\")\n",
        "\n",
        "def get_documents_by_ids(doc_df: pd.DataFrame, doc_ids: List[int]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve document passages by their IDs.\n",
        "    \n",
        "    Args:\n",
        "        doc_df: DataFrame containing documents\n",
        "        doc_ids: List of document IDs to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        List of document passages\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for doc_id in doc_ids:\n",
        "        doc = doc_df[doc_df['id'] == doc_id]\n",
        "        if not doc.empty:\n",
        "            documents.append(doc['passage'].values[0])\n",
        "        else:\n",
        "            print(f\"Warning: Document ID {doc_id} not found\")\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rag_prompt(question: str, documents: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Create a RAG prompt with question and retrieved documents.\n",
        "    \n",
        "    Args:\n",
        "        question: The user's question\n",
        "        documents: List of retrieved document passages\n",
        "    \n",
        "    Returns:\n",
        "        Formatted prompt string\n",
        "    \"\"\"\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(documents)])\n",
        "    \n",
        "    prompt = f\"\"\"You are a precise question-answering assistant. Based on the provided documents, answer the question with ONLY the direct answer. Do not include explanations, context, or additional information.\n",
        "\n",
        "Context Documents:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "- Provide ONLY the direct answer to the question\n",
        "- Do NOT add phrases like \"Based on the documents...\" or \"According to...\"\n",
        "- Do NOT provide explanations or reasoning\n",
        "- If the answer is a single word, number, or short phrase, return only that\n",
        "- If the answer requires a sentence, make it as brief as possible\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_llm(prompt: str, temperature: float = 0.0, max_retries: int = 3, base_delay: float = 1.0) -> str:\n",
        "    \"\"\"\n",
        "    Send prompt to Ollama (local LLM) and get response with retry logic.\n",
        "    \n",
        "    Args:\n",
        "        prompt: The prompt to send\n",
        "        temperature: Sampling temperature (0.0 for deterministic)\n",
        "        max_retries: Maximum number of retry attempts  \n",
        "        base_delay: Base delay between retries (will use exponential backoff)\n",
        "    \n",
        "    Returns:\n",
        "        Generated answer from LLM, or None if all retries fail\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = ollama.generate(\n",
        "                model=OLLAMA_MODEL,\n",
        "                prompt=prompt,\n",
        "                options={\n",
        "                    'temperature': temperature,\n",
        "                    'num_predict': 500,  # Max tokens\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            # Extract the response text\n",
        "            answer = response.get('response', '').strip()\n",
        "            return answer if answer else None\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            \n",
        "            # Check if Ollama is not running\n",
        "            if \"connection\" in error_msg.lower() or \"refused\" in error_msg.lower():\n",
        "                print(f\"‚ùå Cannot connect to Ollama. Make sure it's running: ollama serve\")\n",
        "                return None\n",
        "            \n",
        "            # Generic error with retry\n",
        "            print(f\"‚ö†Ô∏è  Error calling Ollama (attempt {attempt + 1}/{max_retries}): {error_msg}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = base_delay * (2 ** attempt)\n",
        "                print(f\"   Retrying in {wait_time:.1f}s...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"‚ùå Failed after {max_retries} attempts\")\n",
        "                return None\n",
        "    \n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize answer text for comparison.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to normalize\n",
        "    \n",
        "    Returns:\n",
        "        Normalized text\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower().strip()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def exact_match_score(predicted: str, ground_truth: str) -> bool:\n",
        "    \"\"\"\n",
        "    Calculate exact match score (normalized).\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "    \n",
        "    Returns:\n",
        "        True if answers match exactly (after normalization)\n",
        "    \"\"\"\n",
        "    return normalize_answer(predicted) == normalize_answer(ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contains_answer_score(predicted: str, ground_truth: str) -> bool:\n",
        "    \"\"\"\n",
        "    Check if predicted answer contains the ground truth.\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "    \n",
        "    Returns:\n",
        "        True if predicted contains ground truth\n",
        "    \"\"\"\n",
        "    return normalize_answer(ground_truth) in normalize_answer(predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f1_score(predicted: str, ground_truth: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculate token-level F1 score.\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "    \n",
        "    Returns:\n",
        "        F1 score (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    pred_tokens = set(normalize_answer(predicted).split())\n",
        "    gt_tokens = set(normalize_answer(ground_truth).split())\n",
        "    \n",
        "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    common_tokens = pred_tokens.intersection(gt_tokens)\n",
        "    \n",
        "    if len(common_tokens) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(gt_tokens)\n",
        "    \n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You created embeddings with 4 different models in `Embedding_process.ipynb`:\n",
        "- `bert-base-uncased`\n",
        "- `multi-qa-mpnet-base-dot-v1` ‚Üê **Using this one**\n",
        "- `intfloat/e5-small-v2`\n",
        "- `hkunlp/instructor-large`\n",
        "\n",
        "You can change the model above to test which embedding model gives the best evaluation correlation!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Embedding-based Evaluation Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Use E5 or Instructor models (uncomment to use)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**‚ö†Ô∏è Important Note on Embedding Metrics:**\n",
        "\n",
        "Embedding-based metrics work best for **longer text** (paragraphs, sentences). For **very short answers** like \"yes\" vs \"no\", they can give misleading results:\n",
        "\n",
        "- ‚úÖ **Good for**: Comparing longer answers where semantic meaning is clear\n",
        "- ‚ùå **Not ideal for**: Single-word answers, especially yes/no questions\n",
        "- üí° **Recommendation**: For short answers, rely more on **text-based metrics** (exact match, F1 score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Option 1: E5 model (requires \"query: \" prefix for answers)\n",
        "# embedding_model = SentenceTransformer('intfloat/e5-small-v2')\n",
        "# \n",
        "# def compute_embedding_metrics_e5(predicted: str, ground_truth: str) -> Dict[str, float]:\n",
        "#     # Add \"query: \" prefix as done in Embedding_process.ipynb\n",
        "#     emb1 = embedding_model.encode([\"query: \" + predicted])[0]\n",
        "#     emb2 = embedding_model.encode([\"query: \" + ground_truth])[0]\n",
        "#     \n",
        "#     from sklearn.preprocessing import normalize\n",
        "#     emb1 = normalize([emb1])[0]\n",
        "#     emb2 = normalize([emb2])[0]\n",
        "#     \n",
        "#     return {\n",
        "#         'cosine_similarity': float(np.dot(emb1, emb2)),\n",
        "#         'euclidean_distance': float(np.linalg.norm(emb1 - emb2)),\n",
        "#         'dot_product_similarity': float(np.dot(emb1, emb2)),\n",
        "#         'manhattan_distance': float(np.sum(np.abs(emb1 - emb2)))\n",
        "#     }\n",
        "\n",
        "# # Option 2: Instructor model (requires instruction)\n",
        "# from InstructorEmbedding import INSTRUCTOR\n",
        "# embedding_model = INSTRUCTOR('hkunlp/instructor-large')\n",
        "# \n",
        "# def compute_embedding_metrics_instructor(predicted: str, ground_truth: str) -> Dict[str, float]:\n",
        "#     # Add instruction as done in Embedding_process.ipynb\n",
        "#     emb1 = embedding_model.encode([[\"Represent the answer:\", predicted]])[0]\n",
        "#     emb2 = embedding_model.encode([[\"Represent the answer:\", ground_truth]])[0]\n",
        "#     \n",
        "#     # Note: Instructor embeddings are NOT normalized in Embedding_process.ipynb\n",
        "#     return {\n",
        "#         'cosine_similarity': float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))),\n",
        "#         'euclidean_distance': float(np.linalg.norm(emb1 - emb2)),\n",
        "#         'dot_product_similarity': float(np.dot(emb1, emb2)),\n",
        "#         'manhattan_distance': float(np.sum(np.abs(emb1 - emb2)))\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_embedding_metrics(predicted: str, ground_truth: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute multiple embedding-based metrics between predicted and ground truth answers.\n",
        "    \n",
        "    Metrics:\n",
        "    - Cosine Similarity: Measures angular similarity (0 to 1, higher is better)\n",
        "    - Euclidean Distance: L2 distance between embeddings (lower is better)\n",
        "    - Dot Product: Inner product of normalized embeddings (0 to 1, higher is better)\n",
        "    - Manhattan Distance: L1 distance between embeddings (lower is better)\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with embedding-based metrics\n",
        "    \"\"\"\n",
        "    # Generate embeddings\n",
        "    emb1 = embedding_model.encode([predicted])[0]\n",
        "    emb2 = embedding_model.encode([ground_truth])[0]\n",
        "    \n",
        "    # Normalize embeddings (same as in Embedding_process.ipynb)\n",
        "    from sklearn.preprocessing import normalize\n",
        "    emb1 = normalize([emb1])[0]\n",
        "    emb2 = normalize([emb2])[0]\n",
        "    \n",
        "    # Cosine similarity (for normalized vectors, this equals dot product)\n",
        "    cos_sim = np.dot(emb1, emb2)\n",
        "    \n",
        "    # Euclidean distance\n",
        "    euclidean_dist = np.linalg.norm(emb1 - emb2)\n",
        "    \n",
        "    # Dot product similarity (same as cosine for normalized embeddings)\n",
        "    dot_product = np.dot(emb1, emb2)\n",
        "    \n",
        "    # Manhattan distance\n",
        "    manhattan_dist = np.sum(np.abs(emb1 - emb2))\n",
        "    \n",
        "    return {\n",
        "        'cosine_similarity': float(cos_sim),\n",
        "        'euclidean_distance': float(euclidean_dist),\n",
        "        'dot_product_similarity': float(dot_product),\n",
        "        'manhattan_distance': float(manhattan_dist)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Understanding the Metrics\n",
        "\n",
        "**For SHORT answers (yes/no, single words):**\n",
        "- ‚úÖ **Exact Match** - Most reliable! 0 or 1, no ambiguity\n",
        "- ‚úÖ **F1 Score** - Good for token overlap\n",
        "- ‚ùå **Embedding Metrics** - Can be misleading (e.g., \"yes\" vs \"no\" might have high similarity)\n",
        "\n",
        "**For LONG answers (sentences, paragraphs):**\n",
        "- ‚úÖ **Embedding Metrics** - Excellent! Captures semantic meaning\n",
        "- ‚úÖ **F1 Score** - Good for word overlap\n",
        "- ‚ö†Ô∏è **Exact Match** - Too strict, rarely matches\n",
        "\n",
        "**Example of the issue:**\n",
        "```\n",
        "Ground Truth: \"yes\"\n",
        "Predicted: \"no\"\n",
        "Exact Match: 0.0 ‚úì (Correct - they're different)\n",
        "Cosine Similarity: 0.933 ‚úó (Misleading - seems similar but they're opposite!)\n",
        "```\n",
        "\n",
        "**Why?** Embeddings capture that both are short, single-word boolean answers in similar contexts, not that they're semantically opposite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Recommendations\n",
        "\n",
        "**Option 1: Filter your analysis**\n",
        "When analyzing results, separate short vs long answers:\n",
        "```python\n",
        "# After getting results_df\n",
        "short_answers = results_df[results_df['ground_truth'].str.split().str.len() <= 2]\n",
        "long_answers = results_df[results_df['ground_truth'].str.split().str.len() > 2]\n",
        "\n",
        "# For short answers: focus on exact_match and f1_score\n",
        "# For long answers: embedding metrics are more reliable\n",
        "```\n",
        "\n",
        "**Option 2: Use text metrics primarily**\n",
        "For this RAG evaluation, **Exact Match** and **F1 Score** are your most reliable metrics across all answer lengths.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. RAG Evaluation Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_rag_single(\n",
        "    question_id: int,\n",
        "    doc_ids: List[int],\n",
        "    qa_df: pd.DataFrame,\n",
        "    doc_df: pd.DataFrame,\n",
        "    use_embedding_metrics: bool = True,\n",
        "    verbose: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate RAG for a single question.\n",
        "    \n",
        "    Args:\n",
        "        question_id: ID of the question in qa_df\n",
        "        doc_ids: List of document IDs to use for context\n",
        "        qa_df: DataFrame with questions and answers\n",
        "        doc_df: DataFrame with documents\n",
        "        use_embedding_metrics: Whether to compute embedding-based metrics\n",
        "        verbose: Whether to print detailed output\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with evaluation results\n",
        "    \"\"\"\n",
        "    # Get question and ground truth\n",
        "    qa_row = qa_df[qa_df['id'] == question_id]\n",
        "    if qa_row.empty:\n",
        "        print(f\"Error: Question ID {question_id} not found\")\n",
        "        return None\n",
        "    \n",
        "    question = qa_row['question'].values[0]\n",
        "    ground_truth = qa_row['answer'].values[0]\n",
        "    \n",
        "    # Retrieve documents\n",
        "    documents = get_documents_by_ids(doc_df, doc_ids)\n",
        "    \n",
        "    if not documents:\n",
        "        print(\"Error: No documents retrieved\")\n",
        "        return None\n",
        "    \n",
        "    # Create prompt\n",
        "    prompt = create_rag_prompt(question, documents)\n",
        "    \n",
        "    # Query LLM\n",
        "    predicted_answer = query_llm(prompt)\n",
        "    \n",
        "    if predicted_answer is None:\n",
        "        print(\"Error: Failed to get LLM response\")\n",
        "        return None\n",
        "    \n",
        "    # Calculate text-based metrics\n",
        "    exact_match = exact_match_score(predicted_answer, ground_truth)\n",
        "    contains = contains_answer_score(predicted_answer, ground_truth)\n",
        "    f1 = f1_score(predicted_answer, ground_truth)\n",
        "    \n",
        "    # Prepare results\n",
        "    results = {\n",
        "        'question_id': question_id,\n",
        "        'question': question,\n",
        "        'ground_truth': ground_truth,\n",
        "        'predicted_answer': predicted_answer,\n",
        "        'doc_ids': doc_ids,\n",
        "        'num_docs': len(documents),\n",
        "        'exact_match': exact_match,\n",
        "        'contains_answer': contains,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "    \n",
        "    # Calculate embedding-based metrics\n",
        "    if use_embedding_metrics:\n",
        "        embedding_metrics = compute_embedding_metrics(predicted_answer, ground_truth)\n",
        "        results.update(embedding_metrics)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Question ID: {question_id}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"\\nRetrieved Documents: {doc_ids}\")\n",
        "        print(f\"Number of Documents: {len(documents)}\")\n",
        "        print(f\"\\nGround Truth: {ground_truth}\")\n",
        "        print(f\"Predicted Answer: {predicted_answer}\")\n",
        "        print(f\"\\nText-based Metrics:\")\n",
        "        print(f\"  Exact Match: {exact_match}\")\n",
        "        print(f\"  Contains Answer: {contains}\")\n",
        "        print(f\"  F1 Score: {f1:.3f}\")\n",
        "        \n",
        "        if use_embedding_metrics:\n",
        "            print(f\"\\nEmbedding-based Metrics:\")\n",
        "            print(f\"  Cosine Similarity: {results['cosine_similarity']:.4f}\")\n",
        "            print(f\"  Euclidean Distance: {results['euclidean_distance']:.4f}\")\n",
        "            print(f\"  Dot Product Similarity: {results['dot_product_similarity']:.4f}\")\n",
        "            print(f\"  Manhattan Distance: {results['manhattan_distance']:.4f}\")\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Single Example Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Question ID: 0\n",
            "Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
            "\n",
            "Retrieved Documents: [1, 2, 3]\n",
            "Number of Documents: 3\n",
            "\n",
            "Ground Truth: yes\n",
            "Predicted Answer: No.\n",
            "\n",
            "Text-based Metrics:\n",
            "  Exact Match: False\n",
            "  Contains Answer: False\n",
            "  F1 Score: 0.000\n",
            "\n",
            "Embedding-based Metrics:\n",
            "  Cosine Similarity: 0.9336\n",
            "  Euclidean Distance: 0.3644\n",
            "  Dot Product Similarity: 0.9336\n",
            "  Manhattan Distance: 8.2146\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Example: Evaluate a single question with selected documents\n",
        "# Replace question_id and doc_ids with your actual values\n",
        "\n",
        "question_id = 0  # Question ID from qa_df\n",
        "doc_ids = [1, 2, 3]  # Document IDs you determined are best for this question\n",
        "\n",
        "result = evaluate_rag_single(\n",
        "    question_id=question_id,\n",
        "    doc_ids=doc_ids,\n",
        "    qa_df=qa_df,\n",
        "    doc_df=doc_df,\n",
        "    use_embedding_metrics=True,  # Enable embedding-based metrics\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Batch Evaluation\n",
        "\n",
        "**‚úÖ Benefits of Using Ollama (Local LLM):**\n",
        "- **No rate limits!** Run as many evaluations as you want\n",
        "- **No API costs** - completely free\n",
        "- **Privacy** - all processing happens locally\n",
        "- **Fast** - no network latency\n",
        "\n",
        "**Features:**\n",
        "- **Checkpoint saving** every 5 questions (auto-resumes on restart)\n",
        "- **Automatic retries** if something goes wrong\n",
        "- **Progress tracking** with clear status messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_rag_batch(\n",
        "    question_doc_pairs: List[Tuple[int, List[int]]],\n",
        "    qa_df: pd.DataFrame,\n",
        "    doc_df: pd.DataFrame,\n",
        "    use_embedding_metrics: bool = True,\n",
        "    verbose: bool = False,\n",
        "    rate_limit_delay: float = 1.0,\n",
        "    checkpoint_file: str = \"data_processed/rag_evaluation_checkpoint.csv\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate RAG for multiple questions with rate limiting and progress saving.\n",
        "    \n",
        "    Args:\n",
        "        question_doc_pairs: List of (question_id, doc_ids) tuples\n",
        "        qa_df: DataFrame with questions and answers\n",
        "        doc_df: DataFrame with documents\n",
        "        use_embedding_metrics: Whether to compute embedding-based metrics\n",
        "        verbose: Whether to print detailed output for each question\n",
        "        rate_limit_delay: Seconds to wait between API calls (helps avoid rate limits)\n",
        "        checkpoint_file: File to save progress (in case of interruption)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with evaluation results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Try to load existing checkpoint\n",
        "    import os\n",
        "    checkpoint_exists = os.path.exists(checkpoint_file)\n",
        "    processed_ids = set()\n",
        "    \n",
        "    if checkpoint_exists:\n",
        "        try:\n",
        "            checkpoint_df = pd.read_csv(checkpoint_file)\n",
        "            results = checkpoint_df.to_dict('records')\n",
        "            processed_ids = set(checkpoint_df['question_id'].values)\n",
        "            print(f\"üìÇ Loaded checkpoint: {len(processed_ids)} questions already processed\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not load checkpoint: {e}\")\n",
        "    \n",
        "    for i, (question_id, doc_ids) in enumerate(question_doc_pairs):\n",
        "        # Skip if already processed\n",
        "        if question_id in processed_ids:\n",
        "            print(f\"‚è≠Ô∏è  Skipping question {i+1}/{len(question_doc_pairs)} (ID: {question_id}) - already processed\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"üîÑ Evaluating question {i+1}/{len(question_doc_pairs)} (ID: {question_id})...\")\n",
        "        \n",
        "        # Add delay to avoid rate limits (except for first request)\n",
        "        if len(results) > 0:\n",
        "            time.sleep(rate_limit_delay)\n",
        "        \n",
        "        result = evaluate_rag_single(\n",
        "            question_id=question_id,\n",
        "            doc_ids=doc_ids,\n",
        "            qa_df=qa_df,\n",
        "            doc_df=doc_df,\n",
        "            use_embedding_metrics=use_embedding_metrics,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        \n",
        "        if result:\n",
        "            results.append(result)\n",
        "            processed_ids.add(question_id)\n",
        "            \n",
        "            # Save checkpoint every 5 questions\n",
        "            if len(results) % 5 == 0:\n",
        "                try:\n",
        "                    checkpoint_df = pd.DataFrame(results)\n",
        "                    os.makedirs(os.path.dirname(checkpoint_file), exist_ok=True)\n",
        "                    checkpoint_df.to_csv(checkpoint_file, index=False)\n",
        "                    print(f\"üíæ Checkpoint saved: {len(results)} questions completed\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Could not save checkpoint: {e}\")\n",
        "    \n",
        "    # Final save\n",
        "    results_df = pd.DataFrame(results)\n",
        "    if len(results) > 0:\n",
        "        try:\n",
        "            results_df.to_csv(checkpoint_file, index=False)\n",
        "            print(f\"üíæ Final checkpoint saved\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not save final checkpoint: {e}\")\n",
        "    \n",
        "    # Calculate aggregate metrics\n",
        "    if len(results_df) > 0:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"AGGREGATE RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Total Questions Evaluated: {len(results_df)}\")\n",
        "        print(f\"\\nText-based Metrics:\")\n",
        "        print(f\"  Exact Match Accuracy: {results_df['exact_match'].mean():.3f}\")\n",
        "        print(f\"  Contains Answer Accuracy: {results_df['contains_answer'].mean():.3f}\")\n",
        "        print(f\"  Average F1 Score: {results_df['f1_score'].mean():.3f}\")\n",
        "        \n",
        "        if use_embedding_metrics and 'cosine_similarity' in results_df.columns:\n",
        "            print(f\"\\nEmbedding-based Metrics:\")\n",
        "            print(f\"  Average Cosine Similarity: {results_df['cosine_similarity'].mean():.4f}\")\n",
        "            print(f\"  Average Euclidean Distance: {results_df['euclidean_distance'].mean():.4f}\")\n",
        "            print(f\"  Average Dot Product Similarity: {results_df['dot_product_similarity'].mean():.4f}\")\n",
        "            print(f\"  Average Manhattan Distance: {results_df['manhattan_distance'].mean():.4f}\")\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No results to display\")\n",
        "    \n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example batch evaluation\n",
        "# Format: List of (question_id, doc_ids) tuples\n",
        "question_doc_pairs = [\n",
        "    (0, [0, 1, 2]),      # Question 0 with documents [0, 1, 2]\n",
        "    (2, [3, 4, 5]),      # Question 2 with documents [3, 4, 5]\n",
        "    (4, [6, 7, 8]),      # Question 4 with documents [6, 7, 8]\n",
        "]\n",
        "\n",
        "results_df = evaluate_rag_batch(\n",
        "    question_doc_pairs=question_doc_pairs,\n",
        "    qa_df=qa_df,\n",
        "    doc_df=doc_df,\n",
        "    use_embedding_metrics=True,  # Enable embedding-based metrics\n",
        "    verbose=False,\n",
        "    rate_limit_delay=0.5,  # Small delay for stability (Ollama has no rate limits!)\n",
        "    checkpoint_file=\"data_processed/rag_evaluation_checkpoint.csv\"  # Save progress\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clear Checkpoint (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to clear checkpoint and start fresh\n",
        "# import os\n",
        "# checkpoint_file = \"data_processed/rag_evaluation_checkpoint.csv\"\n",
        "# if os.path.exists(checkpoint_file):\n",
        "#     os.remove(checkpoint_file)\n",
        "#     print(\"üóëÔ∏è  Checkpoint cleared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results table with all metrics\n",
        "results_df[['question_id', 'question', 'ground_truth', 'predicted_answer', \n",
        "            'exact_match', 'contains_answer', 'f1_score',\n",
        "            'cosine_similarity', 'euclidean_distance', 'dot_product_similarity']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results to CSV with embedding metrics\n",
        "results_df.to_csv('data_processed/rag_evaluation_results_with_embeddings.csv', index=False)\n",
        "print(\"Results saved to data_processed/rag_evaluation_results_with_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Analyze Embedding vs Text Metrics (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze correlation between embedding similarity and text-based metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot: Cosine Similarity vs F1 Score\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(results_df['cosine_similarity'], results_df['f1_score'], alpha=0.6, s=100)\n",
        "plt.xlabel('Cosine Similarity (Embedding-based)', fontsize=12)\n",
        "plt.ylabel('F1 Score (Text-based)', fontsize=12)\n",
        "plt.title('Embedding Similarity vs Text-based Similarity', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print correlation\n",
        "correlation = results_df['cosine_similarity'].corr(results_df['f1_score'])\n",
        "print(f\"\\nCorrelation between Cosine Similarity and F1 Score: {correlation:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
