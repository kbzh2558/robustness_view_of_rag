{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG LLM Evaluation with Embedding Metrics\n",
        "\n",
        "This notebook evaluates RAG performance by:\n",
        "1. Taking a question and selected document IDs\n",
        "2. Creating prompts with retrieved documents\n",
        "3. Passing to an LLM (**Ollama - runs locally, no API limits!**)\n",
        "4. Comparing generated answers with ground truth using:\n",
        "   - **Text-based metrics** (exact match, F1 score)\n",
        "   - **Embedding-based metrics** (cosine similarity, euclidean distance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/laurenzhang/Desktop/MIT/15.C57/robustness_view_of_rag/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import ollama\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "from bert_score import score as bert_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 918 questions\n",
            "                                            question     answer  id\n",
            "0  Was Abraham Lincoln the sixteenth President of...        yes   0\n",
            "1  Did Lincoln sign the National Banking Act of 1...        yes   2\n",
            "2                   Did his mother die of pneumonia?         no   4\n",
            "3      How many long was Lincoln's formal education?  18 months   6\n",
            "4       When did Lincoln begin his political career?       1832   8\n"
          ]
        }
      ],
      "source": [
        "# Load Q&A data\n",
        "qa_df = pd.read_csv('data/rag-mini-wikipedia_q_and_a.csv')\n",
        "print(f\"Loaded {len(qa_df)} questions\")\n",
        "print(qa_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 3200 documents\n",
            "                                             passage  id\n",
            "0  Uruguay (official full name in  ; pron.  , Eas...   0\n",
            "1  It is bordered by Brazil to the north, by Arge...   1\n",
            "2  Montevideo was founded by the Spanish in the e...   2\n",
            "3  The economy is largely based in agriculture (m...   3\n",
            "4  According to Transparency International, Urugu...   4\n"
          ]
        }
      ],
      "source": [
        "# Load document data\n",
        "doc_df = pd.read_csv('data/rag-mini-wikipedia_document.csv')\n",
        "print(f\"Loaded {len(doc_df)} documents\")\n",
        "print(doc_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setup Ollama (Local LLM)\n",
        "\n",
        "**‚úÖ Ollama is already running on your system!**\n",
        "\n",
        "You have these models installed:\n",
        "- `llama3` (8B params) ‚Üê **Using this one**\n",
        "- `gemma3` (3B params)\n",
        "\n",
        "**To use a different model:**\n",
        "```bash\n",
        "# Change OLLAMA_MODEL in the cell below to one of:\n",
        "OLLAMA_MODEL = \"llama3\"   # Currently selected\n",
        "OLLAMA_MODEL = \"gemma3\"   # Alternative option\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Ollama is running!\n",
            "üì¶ Available models:\n",
            "   - ('models', [Model(model='llama3:latest', modified_at=datetime.datetime(2025, 10, 7, 13, 18, 0, 391725, tzinfo=TzInfo(-14400)), digest='365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1', size=4661224676, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0')), Model(model='gemma3:latest', modified_at=datetime.datetime(2025, 10, 7, 12, 9, 38, 308655, tzinfo=TzInfo(-14400)), digest='a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a', size=3338801804, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M'))])\n",
            "\n",
            "‚úì Using model: llama3\n"
          ]
        }
      ],
      "source": [
        "# Configure Ollama\n",
        "OLLAMA_MODEL = \"llama3\"  # Using your installed model\n",
        "\n",
        "# Test connection to Ollama\n",
        "try:\n",
        "    # List available models\n",
        "    models_response = ollama.list()\n",
        "    \n",
        "    # Parse the response - it might be a dict with 'models' key or have different structure\n",
        "    if isinstance(models_response, dict) and 'models' in models_response:\n",
        "        models_list = models_response['models']\n",
        "    else:\n",
        "        models_list = models_response\n",
        "    \n",
        "    # Extract model names - handle different response formats\n",
        "    available_models = []\n",
        "    for model in models_list:\n",
        "        if isinstance(model, dict):\n",
        "            # Try different possible keys\n",
        "            name = model.get('name') or model.get('model') or str(model)\n",
        "            available_models.append(name)\n",
        "        else:\n",
        "            available_models.append(str(model))\n",
        "    \n",
        "    print(f\"‚úì Ollama is running!\")\n",
        "    print(f\"üì¶ Available models:\")\n",
        "    for model in available_models:\n",
        "        print(f\"   - {model}\")\n",
        "    \n",
        "    # Check if selected model is available\n",
        "    if any(OLLAMA_MODEL in str(model_name) for model_name in available_models):\n",
        "        print(f\"\\n‚úì Using model: {OLLAMA_MODEL}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Model '{OLLAMA_MODEL}' not found.\")\n",
        "        print(f\"Available: {available_models}\")\n",
        "        print(f\"Run: ollama pull {OLLAMA_MODEL}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
        "    print(f\"\\nDebug info:\")\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nMake sure Ollama is running.\")\n",
        "    print(\"\\nTo start Ollama:\")\n",
        "    print(\"  1. Open the Ollama app from Applications, OR\")\n",
        "    print(\"  2. Run in terminal: ollama serve\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup Embedding Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model loaded: multi-qa-mpnet-base-dot-v1\n"
          ]
        }
      ],
      "source": [
        "# Initialize embedding model for semantic similarity evaluation\n",
        "# Using the SAME model as in Embedding_process.ipynb for consistency\n",
        "embedding_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
        "print(f\"Embedding model loaded: multi-qa-mpnet-base-dot-v1\")\n",
        "\n",
        "def get_documents_by_ids(doc_df: pd.DataFrame, doc_ids: List[int]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve document passages by their IDs.\n",
        "    \n",
        "    Args:\n",
        "        doc_df: DataFrame containing documents\n",
        "        doc_ids: List of document IDs to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        List of document passages\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for doc_id in doc_ids:\n",
        "        doc = doc_df[doc_df['id'] == doc_id]\n",
        "        if not doc.empty:\n",
        "            documents.append(doc['passage'].values[0])\n",
        "        else:\n",
        "            print(f\"Warning: Document ID {doc_id} not found\")\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rag_prompt(question: str, documents: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Create a RAG prompt with question and retrieved documents.\n",
        "    \n",
        "    Args:\n",
        "        question: The user's question\n",
        "        documents: List of retrieved document passages\n",
        "    \n",
        "    Returns:\n",
        "        Formatted prompt string\n",
        "    \"\"\"\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(documents)])\n",
        "    \n",
        "    prompt = f\"\"\"You are a precise question-answering assistant. Based on the provided documents, answer the question with ONLY the direct answer. Do not include explanations, context, or additional information.\n",
        "\n",
        "Context Documents:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "- Provide ONLY the direct answer to the question\n",
        "- Do NOT add phrases like \"Based on the documents...\" or \"According to...\"\n",
        "- Do NOT provide explanations or reasoning\n",
        "- If the answer is a single word, number, or short phrase, return only that\n",
        "- If the answer requires a sentence, make it as brief as possible\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_llm(prompt: str, temperature: float = 0.0, max_retries: int = 3, base_delay: float = 1.0) -> str:\n",
        "    \"\"\"\n",
        "    Send prompt to Ollama (local LLM) and get response with retry logic.\n",
        "    \n",
        "    Args:\n",
        "        prompt: The prompt to send\n",
        "        temperature: Sampling temperature (0.0 for deterministic)\n",
        "        max_retries: Maximum number of retry attempts  \n",
        "        base_delay: Base delay between retries (will use exponential backoff)\n",
        "    \n",
        "    Returns:\n",
        "        Generated answer from LLM, or None if all retries fail\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = ollama.generate(\n",
        "                model=OLLAMA_MODEL,\n",
        "                prompt=prompt,\n",
        "                options={\n",
        "                    'temperature': temperature,\n",
        "                    'num_predict': 500,  # Max tokens\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            # Extract the response text\n",
        "            answer = response.get('response', '').strip()\n",
        "            return answer if answer else None\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            \n",
        "            # Check if Ollama is not running\n",
        "            if \"connection\" in error_msg.lower() or \"refused\" in error_msg.lower():\n",
        "                print(f\"‚ùå Cannot connect to Ollama. Make sure it's running: ollama serve\")\n",
        "                return None\n",
        "            \n",
        "            # Generic error with retry\n",
        "            print(f\"‚ö†Ô∏è  Error calling Ollama (attempt {attempt + 1}/{max_retries}): {error_msg}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = base_delay * (2 ** attempt)\n",
        "                print(f\"   Retrying in {wait_time:.1f}s...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"‚ùå Failed after {max_retries} attempts\")\n",
        "                return None\n",
        "    \n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize answer text for comparison.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to normalize\n",
        "    \n",
        "    Returns:\n",
        "        Normalized text\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower().strip()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def exact_match_score(predicted: str, ground_truth: str) -> bool:\n",
        "    \"\"\"\n",
        "    Calculate exact match score (normalized).\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "    \n",
        "    Returns:\n",
        "        True if answers match exactly (after normalization)\n",
        "    \"\"\"\n",
        "    return normalize_answer(predicted) == normalize_answer(ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contains_answer_score(predicted: str, ground_truth: str) -> bool:\n",
        "    \"\"\"\n",
        "    Check if predicted answer contains the ground truth.\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "    \n",
        "    Returns:\n",
        "        True if predicted contains ground truth\n",
        "    \"\"\"\n",
        "    return normalize_answer(ground_truth) in normalize_answer(predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f1_score(predicted: str, ground_truth: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculate token-level F1 score.\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "    \n",
        "    Returns:\n",
        "        F1 score (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    pred_tokens = set(normalize_answer(predicted).split())\n",
        "    gt_tokens = set(normalize_answer(ground_truth).split())\n",
        "    \n",
        "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    common_tokens = pred_tokens.intersection(gt_tokens)\n",
        "    \n",
        "    if len(common_tokens) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(gt_tokens)\n",
        "    \n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You created embeddings with 4 different models in `Embedding_process.ipynb`:\n",
        "- `bert-base-uncased`\n",
        "- `multi-qa-mpnet-base-dot-v1` ‚Üê **Using this one**\n",
        "- `intfloat/e5-small-v2`\n",
        "- `hkunlp/instructor-large`\n",
        "\n",
        "You can change the model above to test which embedding model gives the best evaluation correlation!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Embedding-based Evaluation Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Use E5 or Instructor models (uncomment to use)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**‚ö†Ô∏è Important Note on Embedding Metrics:**\n",
        "\n",
        "Embedding-based metrics work best for **longer text** (paragraphs, sentences). For **very short answers** like \"yes\" vs \"no\", they can give misleading results:\n",
        "\n",
        "- ‚úÖ **Good for**: Comparing longer answers where semantic meaning is clear\n",
        "- ‚ùå **Not ideal for**: Single-word answers, especially yes/no questions\n",
        "- üí° **Recommendation**: For short answers, rely more on **text-based metrics** (exact match, F1 score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Option 1: E5 model (requires \"query: \" prefix for answers)\n",
        "# embedding_model = SentenceTransformer('intfloat/e5-small-v2')\n",
        "# \n",
        "# def compute_embedding_metrics_e5(predicted: str, ground_truth: str) -> Dict[str, float]:\n",
        "#     # Add \"query: \" prefix as done in Embedding_process.ipynb\n",
        "#     emb1 = embedding_model.encode([\"query: \" + predicted])[0]\n",
        "#     emb2 = embedding_model.encode([\"query: \" + ground_truth])[0]\n",
        "#     \n",
        "#     from sklearn.preprocessing import normalize\n",
        "#     emb1 = normalize([emb1])[0]\n",
        "#     emb2 = normalize([emb2])[0]\n",
        "#     \n",
        "#     return {\n",
        "#         'cosine_similarity': float(np.dot(emb1, emb2)),\n",
        "#         'euclidean_distance': float(np.linalg.norm(emb1 - emb2)),\n",
        "#         'dot_product_similarity': float(np.dot(emb1, emb2)),\n",
        "#         'manhattan_distance': float(np.sum(np.abs(emb1 - emb2)))\n",
        "#     }\n",
        "\n",
        "# # Option 2: Instructor model (requires instruction)\n",
        "# from InstructorEmbedding import INSTRUCTOR\n",
        "# embedding_model = INSTRUCTOR('hkunlp/instructor-large')\n",
        "# \n",
        "# def compute_embedding_metrics_instructor(predicted: str, ground_truth: str) -> Dict[str, float]:\n",
        "#     # Add instruction as done in Embedding_process.ipynb\n",
        "#     emb1 = embedding_model.encode([[\"Represent the answer:\", predicted]])[0]\n",
        "#     emb2 = embedding_model.encode([[\"Represent the answer:\", ground_truth]])[0]\n",
        "#     \n",
        "#     # Note: Instructor embeddings are NOT normalized in Embedding_process.ipynb\n",
        "#     return {\n",
        "#         'cosine_similarity': float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))),\n",
        "#         'euclidean_distance': float(np.linalg.norm(emb1 - emb2)),\n",
        "#         'dot_product_similarity': float(np.dot(emb1, emb2)),\n",
        "#         'manhattan_distance': float(np.sum(np.abs(emb1 - emb2)))\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_embedding_metrics(predicted: str, ground_truth: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute multiple embedding-based metrics between predicted and ground truth answers.\n",
        "    \n",
        "    Metrics:\n",
        "    - Cosine Similarity: Measures angular similarity (0 to 1, higher is better)\n",
        "    - Euclidean Distance: L2 distance between embeddings (lower is better)\n",
        "    - Dot Product: Inner product of normalized embeddings (0 to 1, higher is better)\n",
        "    - Manhattan Distance: L1 distance between embeddings (lower is better)\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with embedding-based metrics\n",
        "    \"\"\"\n",
        "    # Generate embeddings\n",
        "    emb1 = embedding_model.encode([predicted])[0]\n",
        "    emb2 = embedding_model.encode([ground_truth])[0]\n",
        "    \n",
        "    # Normalize embeddings (same as in Embedding_process.ipynb)\n",
        "    from sklearn.preprocessing import normalize\n",
        "    emb1 = normalize([emb1])[0]\n",
        "    emb2 = normalize([emb2])[0]\n",
        "    \n",
        "    # Cosine similarity (for normalized vectors, this equals dot product)\n",
        "    cos_sim = np.dot(emb1, emb2)\n",
        "    \n",
        "    # Euclidean distance\n",
        "    euclidean_dist = np.linalg.norm(emb1 - emb2)\n",
        "    \n",
        "    # Dot product similarity (same as cosine for normalized embeddings)\n",
        "    dot_product = np.dot(emb1, emb2)\n",
        "    \n",
        "    # Manhattan distance\n",
        "    manhattan_dist = np.sum(np.abs(emb1 - emb2))\n",
        "    \n",
        "    return {\n",
        "        'cosine_similarity': float(cos_sim),\n",
        "        'euclidean_distance': float(euclidean_dist),\n",
        "        'dot_product_similarity': float(dot_product),\n",
        "        'manhattan_distance': float(manhattan_dist)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Understanding the Metrics\n",
        "\n",
        "**For SHORT answers (yes/no, single words):**\n",
        "- ‚úÖ **Exact Match** - Most reliable! 0 or 1, no ambiguity\n",
        "- ‚úÖ **F1 Score** - Good for token overlap\n",
        "- ‚ùå **Embedding Metrics** - Can be misleading (e.g., \"yes\" vs \"no\" might have high similarity)\n",
        "\n",
        "**For LONG answers (sentences, paragraphs):**\n",
        "- ‚úÖ **Embedding Metrics** - Excellent! Captures semantic meaning\n",
        "- ‚úÖ **F1 Score** - Good for word overlap\n",
        "- ‚ö†Ô∏è **Exact Match** - Too strict, rarely matches\n",
        "\n",
        "**Example of the issue:**\n",
        "```\n",
        "Ground Truth: \"yes\"\n",
        "Predicted: \"no\"\n",
        "Exact Match: 0.0 ‚úì (Correct - they're different)\n",
        "Cosine Similarity: 0.933 ‚úó (Misleading - seems similar but they're opposite!)\n",
        "```\n",
        "\n",
        "**Why?** Embeddings capture that both are short, single-word boolean answers in similar contexts, not that they're semantically opposite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 BERTScore Metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**‚öôÔ∏è BERTScore Model Options:**\n",
        "\n",
        "The default model (`microsoft/deberta-xlarge-mnli`) is the most accurate but slower. For faster computation:\n",
        "\n",
        "```python\n",
        "# Fast (but less accurate)\n",
        "compute_bertscore(predicted, ground_truth, model_type='bert-base-uncased')\n",
        "\n",
        "# Balanced\n",
        "compute_bertscore(predicted, ground_truth, model_type='roberta-large')\n",
        "\n",
        "# Most accurate (default, slower)\n",
        "compute_bertscore(predicted, ground_truth, model_type='microsoft/deberta-xlarge-mnli')\n",
        "```\n",
        "\n",
        "For batch evaluation, consider using the faster model to save time!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_bertscore(predicted: str, ground_truth: str, lang: str = 'en', model_type: str = 'distilbert-base-uncased') -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute BERTScore metrics between predicted and ground truth answers.\n",
        "    \n",
        "    BERTScore leverages pre-trained BERT embeddings and computes token-level similarity\n",
        "    using cosine similarity. It returns precision, recall, and F1 scores.\n",
        "    \n",
        "    - **Precision**: How much of the predicted answer is relevant?\n",
        "    - **Recall**: How much of the ground truth is captured?\n",
        "    - **F1**: Harmonic mean of precision and recall\n",
        "    \n",
        "    Args:\n",
        "        predicted: Predicted answer\n",
        "        ground_truth: Ground truth answer\n",
        "        lang: Language code (default: 'en' for English)\n",
        "        model_type: BERT model to use. Options:\n",
        "            - 'distilbert-base-uncased' (default, fastest, ~250MB)\n",
        "            - 'bert-base-uncased' (fast, ~420MB)\n",
        "            - 'roberta-large' (slower but more accurate, ~1.4GB)\n",
        "            - 'microsoft/deberta-xlarge-mnli' (slowest but most accurate, ~1.5GB)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with BERTScore precision, recall, and F1\n",
        "    \"\"\"\n",
        "    # BERTScore expects lists of predictions and references\n",
        "    P, R, F1 = bert_score([predicted], [ground_truth], lang=lang, model_type=model_type, verbose=False)\n",
        "    \n",
        "    return {\n",
        "        'bertscore_precision': float(P[0]),\n",
        "        'bertscore_recall': float(R[0]),\n",
        "        'bertscore_f1': float(F1[0])\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**‚öôÔ∏è BERTScore Speed Guide:**\n",
        "\n",
        "The default model (`distilbert-base-uncased`) is **much faster** and good enough for most cases:\n",
        "\n",
        "| Model | Speed | Size | Use Case |\n",
        "|-------|-------|------|----------|\n",
        "| `distilbert-base-uncased` ‚úÖ | Fastest (~1 sec) | 250MB | **Default - Recommended** |\n",
        "| `bert-base-uncased` | Fast (~2 sec) | 420MB | Slightly more accurate |\n",
        "| `roberta-large` | Slow (~5 sec) | 1.4GB | High accuracy needed |\n",
        "| `microsoft/deberta-xlarge-mnli` | Very Slow (~10 sec) | 1.5GB | Research-grade only |\n",
        "\n",
        "**üí° Speed Tips:**\n",
        "- **First run is slower** - it downloads the model (one-time delay)\n",
        "- Use DistilBERT for batch evaluations (100+ questions)\n",
        "- If still too slow, set `use_bertscore=False` to disable it\n",
        "- To use a different model: `compute_bertscore(pred, gt, model_type='bert-base-uncased')`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**To disable BERTScore** (if it's too slow for your use case):\n",
        "\n",
        "```python\n",
        "result = evaluate_rag_single(\n",
        "    question_id=10,\n",
        "    doc_ids=[1],\n",
        "    qa_df=qa_df,\n",
        "    doc_df=doc_df,\n",
        "    use_embedding_metrics=True,\n",
        "    use_bertscore=False,  # ‚Üê Disable BERTScore\n",
        "    verbose=True\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Recommendations\n",
        "\n",
        "**Option 1: Filter your analysis**\n",
        "When analyzing results, separate short vs long answers:\n",
        "```python\n",
        "# After getting results_df\n",
        "short_answers = results_df[results_df['ground_truth'].str.split().str.len() <= 2]\n",
        "long_answers = results_df[results_df['ground_truth'].str.split().str.len() > 2]\n",
        "\n",
        "# For short answers: focus on exact_match and f1_score\n",
        "# For long answers: embedding metrics are more reliable\n",
        "```\n",
        "\n",
        "**Option 2: Use text metrics primarily**\n",
        "For this RAG evaluation, **Exact Match** and **F1 Score** are your most reliable metrics across all answer lengths.\n",
        "\n",
        "### üìä About BERTScore\n",
        "\n",
        "**BERTScore** is a learned metric that uses contextualized embeddings from BERT models to evaluate text generation:\n",
        "\n",
        "- **Better than embedding similarity**: Uses token-level matching with contextual embeddings\n",
        "- **Better than F1**: Captures semantic similarity, not just exact word overlap\n",
        "- **Works well for**: Both short and long answers\n",
        "- **Ranges**: All scores are between 0 and 1 (higher is better)\n",
        "  - Precision: How much of the predicted answer is relevant\n",
        "  - Recall: How much of the ground truth is captured\n",
        "  - F1: Harmonic mean (most commonly used)\n",
        "\n",
        "**‚ö†Ô∏è Performance Note:** BERTScore is slower than other metrics (~1 sec per evaluation with DistilBERT). The default uses a fast model, but you can disable it with `use_bertscore=False` if speed is critical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. RAG Evaluation Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_rag_single(\n",
        "    question_id: int,\n",
        "    doc_ids: List[int],\n",
        "    qa_df: pd.DataFrame,\n",
        "    doc_df: pd.DataFrame,\n",
        "    use_embedding_metrics: bool = True,\n",
        "    use_bertscore: bool = True,\n",
        "    verbose: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate RAG for a single question.\n",
        "    \n",
        "    Args:\n",
        "        question_id: ID of the question in qa_df\n",
        "        doc_ids: List of document IDs to use for context\n",
        "        qa_df: DataFrame with questions and answers\n",
        "        doc_df: DataFrame with documents\n",
        "        use_embedding_metrics: Whether to compute embedding-based metrics\n",
        "        use_bertscore: Whether to compute BERTScore metrics\n",
        "        verbose: Whether to print detailed output\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with evaluation results\n",
        "    \"\"\"\n",
        "    # Get question and ground truth\n",
        "    qa_row = qa_df[qa_df['id'] == question_id]\n",
        "    if qa_row.empty:\n",
        "        print(f\"Error: Question ID {question_id} not found\")\n",
        "        return None\n",
        "    \n",
        "    question = qa_row['question'].values[0]\n",
        "    ground_truth = qa_row['answer'].values[0]\n",
        "    \n",
        "    # Retrieve documents\n",
        "    documents = get_documents_by_ids(doc_df, doc_ids)\n",
        "    \n",
        "    if not documents:\n",
        "        print(\"Error: No documents retrieved\")\n",
        "        return None\n",
        "    \n",
        "    # Create prompt\n",
        "    prompt = create_rag_prompt(question, documents)\n",
        "    \n",
        "    # Query LLM\n",
        "    predicted_answer = query_llm(prompt)\n",
        "    \n",
        "    if predicted_answer is None:\n",
        "        print(\"Error: Failed to get LLM response\")\n",
        "        return None\n",
        "    \n",
        "    # Calculate text-based metrics\n",
        "    exact_match = exact_match_score(predicted_answer, ground_truth)\n",
        "    contains = contains_answer_score(predicted_answer, ground_truth)\n",
        "    f1 = f1_score(predicted_answer, ground_truth)\n",
        "    \n",
        "    # Prepare results\n",
        "    results = {\n",
        "        'question_id': question_id,\n",
        "        'question': question,\n",
        "        'ground_truth': ground_truth,\n",
        "        'predicted_answer': predicted_answer,\n",
        "        'doc_ids': doc_ids,\n",
        "        'num_docs': len(documents),\n",
        "        'exact_match': exact_match,\n",
        "        'contains_answer': contains,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "    \n",
        "    # Calculate embedding-based metrics\n",
        "    if use_embedding_metrics:\n",
        "        embedding_metrics = compute_embedding_metrics(predicted_answer, ground_truth)\n",
        "        results.update(embedding_metrics)\n",
        "    \n",
        "    # Calculate BERTScore\n",
        "    if use_bertscore:\n",
        "        bertscore_metrics = compute_bertscore(predicted_answer, ground_truth)\n",
        "        results.update(bertscore_metrics)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Question ID: {question_id}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"\\nRetrieved Documents: {doc_ids}\")\n",
        "        print(f\"Number of Documents: {len(documents)}\")\n",
        "        print(f\"\\nGround Truth: {ground_truth}\")\n",
        "        print(f\"Predicted Answer: {predicted_answer}\")\n",
        "        print(f\"\\nText-based Metrics:\")\n",
        "        print(f\"  Exact Match: {exact_match}\")\n",
        "        print(f\"  Contains Answer: {contains}\")\n",
        "        print(f\"  F1 Score: {f1:.3f}\")\n",
        "        \n",
        "        if use_embedding_metrics:\n",
        "            print(f\"\\nEmbedding-based Metrics:\")\n",
        "            print(f\"  Cosine Similarity: {results['cosine_similarity']:.4f}\")\n",
        "            print(f\"  Euclidean Distance: {results['euclidean_distance']:.4f}\")\n",
        "            print(f\"  Dot Product Similarity: {results['dot_product_similarity']:.4f}\")\n",
        "            print(f\"  Manhattan Distance: {results['manhattan_distance']:.4f}\")\n",
        "        \n",
        "        if use_bertscore:\n",
        "            print(f\"\\nBERTScore Metrics:\")\n",
        "            print(f\"  Precision: {results['bertscore_precision']:.4f}\")\n",
        "            print(f\"  Recall: {results['bertscore_recall']:.4f}\")\n",
        "            print(f\"  F1: {results['bertscore_f1']:.4f}\")\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Single Example Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Question ID: 10\n",
            "Question: What did The Legal Tender Act of 1862 establish?\n",
            "\n",
            "Retrieved Documents: [1]\n",
            "Number of Documents: 1\n",
            "\n",
            "Ground Truth: the United States Note, the first paper currency in United States history\n",
            "Predicted Answer: Greenback currency.\n",
            "\n",
            "Text-based Metrics:\n",
            "  Exact Match: False\n",
            "  Contains Answer: False\n",
            "  F1 Score: 0.182\n",
            "\n",
            "Embedding-based Metrics:\n",
            "  Cosine Similarity: 0.6025\n",
            "  Euclidean Distance: 0.8916\n",
            "  Dot Product Similarity: 0.6025\n",
            "  Manhattan Distance: 19.2048\n",
            "\n",
            "BERTScore Metrics:\n",
            "  Precision: 0.7555\n",
            "  Recall: 0.6857\n",
            "  F1: 0.7189\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Example: Evaluate a single question with selected documents\n",
        "# Replace question_id and doc_ids with your actual values\n",
        "\n",
        "question_id = 10  # Question ID from qa_df\n",
        "doc_ids = [1]  # Document IDs you determined are best for this question\n",
        "\n",
        "result = evaluate_rag_single(\n",
        "    question_id=question_id,\n",
        "    doc_ids=doc_ids,\n",
        "    qa_df=qa_df,\n",
        "    doc_df=doc_df,\n",
        "    use_embedding_metrics=True,  # Enable embedding-based metrics\n",
        "    use_bertscore=True,  # Enable BERTScore metrics\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Batch Evaluation from CSV\n",
        "\n",
        "If you have a CSV file with retrieval results (question IDs and their retrieved document IDs), you can evaluate all of them at once!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_doc_ids(doc_ids_str) -> List[int]:\n",
        "    \"\"\"\n",
        "    Parse document IDs from various string formats.\n",
        "    \n",
        "    Handles formats like:\n",
        "    - \"[1, 2, 3]\" (list format)\n",
        "    - \"1, 2, 3\" (comma-separated)\n",
        "    - \"1 2 3\" (space-separated)\n",
        "    - \"1\" (single ID)\n",
        "    \n",
        "    Args:\n",
        "        doc_ids_str: String or list of document IDs\n",
        "    \n",
        "    Returns:\n",
        "        List of integer document IDs\n",
        "    \"\"\"\n",
        "    # If already a list, convert to integers and return\n",
        "    if isinstance(doc_ids_str, list):\n",
        "        return [int(x) for x in doc_ids_str]\n",
        "    \n",
        "    # Convert to string and clean up\n",
        "    doc_ids_str = str(doc_ids_str).strip()\n",
        "    \n",
        "    # Remove brackets if present\n",
        "    doc_ids_str = doc_ids_str.replace('[', '').replace(']', '')\n",
        "    \n",
        "    # Try comma-separated first\n",
        "    if ',' in doc_ids_str:\n",
        "        return [int(x.strip()) for x in doc_ids_str.split(',') if x.strip()]\n",
        "    \n",
        "    # Try space-separated\n",
        "    if ' ' in doc_ids_str:\n",
        "        return [int(x.strip()) for x in doc_ids_str.split() if x.strip()]\n",
        "    \n",
        "    # Single ID\n",
        "    return [int(doc_ids_str)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_rag_batch(\n",
        "    retrieval_results_df: pd.DataFrame,\n",
        "    qa_df: pd.DataFrame,\n",
        "    doc_df: pd.DataFrame,\n",
        "    question_id_col: str = 'question_id',\n",
        "    doc_ids_col: str = 'document_idx',\n",
        "    use_embedding_metrics: bool = True,\n",
        "    use_bertscore: bool = True,\n",
        "    verbose: bool = False,\n",
        "    show_progress: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate RAG for multiple questions from a CSV file.\n",
        "    \n",
        "    The question_id column in retrieval_results_df should match the 'id' column in qa_df.\n",
        "    The output DataFrame will contain ALL original columns from retrieval_results_df plus new\n",
        "    columns with evaluation results.\n",
        "    \n",
        "    Args:\n",
        "        retrieval_results_df: DataFrame with question_id and doc_ids columns\n",
        "        qa_df: DataFrame with questions and answers (will be looked up by 'id' column)\n",
        "        doc_df: DataFrame with documents\n",
        "        question_id_col: Name of the column containing question IDs (default: 'question_id')\n",
        "        doc_ids_col: Name of the column containing document IDs (default: 'document_idx')\n",
        "        use_embedding_metrics: Whether to compute embedding-based metrics\n",
        "        use_bertscore: Whether to compute BERTScore metrics\n",
        "        verbose: Whether to print detailed output for each question\n",
        "        show_progress: Whether to show progress updates\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with ALL original columns + evaluation results columns:\n",
        "        - question, ground_truth, predicted_answer\n",
        "        - num_docs, exact_match, contains_answer, f1_score\n",
        "        - [optional] cosine_similarity, euclidean_distance, etc.\n",
        "        - [optional] bertscore_precision, bertscore_recall, bertscore_f1\n",
        "    \"\"\"\n",
        "    results_list = []\n",
        "    total = len(retrieval_results_df)\n",
        "    \n",
        "    for row_idx, row in retrieval_results_df.iterrows():\n",
        "        # Get question index from the specified column\n",
        "        question_idx = int(row[question_id_col])\n",
        "        \n",
        "        doc_ids_str = row[doc_ids_col]\n",
        "        \n",
        "        # Parse document IDs\n",
        "        try:\n",
        "            doc_ids = parse_doc_ids(doc_ids_str)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not parse doc_ids for question at index {question_idx}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Show progress\n",
        "        if show_progress and not verbose:\n",
        "            if (row_idx + 1) % 10 == 0 or (row_idx + 1) == total:\n",
        "                print(f\"Progress: {row_idx + 1}/{total} questions evaluated...\")\n",
        "        \n",
        "        # Get question and ground truth by ID from qa_df\n",
        "        try:\n",
        "            qa_row = qa_df[qa_df['id'] == question_idx]\n",
        "            if qa_row.empty:\n",
        "                print(f\"‚ö†Ô∏è  Warning: Question ID {question_idx} not found in qa_df\")\n",
        "                continue\n",
        "            question = qa_row['question'].values[0]\n",
        "            ground_truth = qa_row['answer'].values[0]\n",
        "        except (KeyError, IndexError) as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not find question with id={question_idx} in qa_df: {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Retrieve documents\n",
        "        documents = get_documents_by_ids(doc_df, doc_ids)\n",
        "        \n",
        "        if not documents:\n",
        "            print(f\"‚ö†Ô∏è  Warning: No documents retrieved for question {question_idx}\")\n",
        "            continue\n",
        "        \n",
        "        # Create prompt\n",
        "        prompt = create_rag_prompt(question, documents)\n",
        "        \n",
        "        # Query LLM\n",
        "        predicted_answer = query_llm(prompt)\n",
        "        \n",
        "        if predicted_answer is None:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Failed to get LLM response for question {question_idx}\")\n",
        "            continue\n",
        "        \n",
        "        # Calculate text-based metrics\n",
        "        exact_match = exact_match_score(predicted_answer, ground_truth)\n",
        "        contains = contains_answer_score(predicted_answer, ground_truth)\n",
        "        f1 = f1_score(predicted_answer, ground_truth)\n",
        "        \n",
        "        # Start with all original columns from the input CSV\n",
        "        result = row.to_dict()\n",
        "        \n",
        "        # Add evaluation results as new columns\n",
        "        result['question'] = question\n",
        "        result['ground_truth'] = ground_truth\n",
        "        result['predicted_answer'] = predicted_answer\n",
        "        result['num_docs'] = len(documents)\n",
        "        result['exact_match'] = exact_match\n",
        "        result['contains_answer'] = contains\n",
        "        result['f1_score'] = f1\n",
        "        \n",
        "        # Calculate embedding-based metrics\n",
        "        if use_embedding_metrics:\n",
        "            embedding_metrics = compute_embedding_metrics(predicted_answer, ground_truth)\n",
        "            result.update(embedding_metrics)\n",
        "        \n",
        "        # Calculate BERTScore\n",
        "        if use_bertscore:\n",
        "            bertscore_metrics = compute_bertscore(predicted_answer, ground_truth)\n",
        "            result.update(bertscore_metrics)\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Question Index: {question_idx}\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"\\nRetrieved Documents: {doc_ids}\")\n",
        "            print(f\"Number of Documents: {len(documents)}\")\n",
        "            print(f\"\\nGround Truth: {ground_truth}\")\n",
        "            print(f\"Predicted Answer: {predicted_answer}\")\n",
        "            print(f\"\\nText-based Metrics:\")\n",
        "            print(f\"  Exact Match: {exact_match}\")\n",
        "            print(f\"  Contains Answer: {contains}\")\n",
        "            print(f\"  F1 Score: {f1:.3f}\")\n",
        "            \n",
        "            if use_embedding_metrics:\n",
        "                print(f\"\\nEmbedding-based Metrics:\")\n",
        "                print(f\"  Cosine Similarity: {result['cosine_similarity']:.4f}\")\n",
        "                print(f\"  Euclidean Distance: {result['euclidean_distance']:.4f}\")\n",
        "                print(f\"  Dot Product Similarity: {result['dot_product_similarity']:.4f}\")\n",
        "                print(f\"  Manhattan Distance: {result['manhattan_distance']:.4f}\")\n",
        "            \n",
        "            if use_bertscore:\n",
        "                print(f\"\\nBERTScore Metrics:\")\n",
        "                print(f\"  Precision: {result['bertscore_precision']:.4f}\")\n",
        "                print(f\"  Recall: {result['bertscore_recall']:.4f}\")\n",
        "                print(f\"  F1: {result['bertscore_f1']:.4f}\")\n",
        "            \n",
        "            print(\"=\"*80)\n",
        "        \n",
        "        results_list.append(result)\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    \n",
        "    if show_progress:\n",
        "        print(f\"\\n‚úì Completed! Evaluated {len(results_df)} questions.\")\n",
        "    \n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**üìã Note about Output Format:**\n",
        "\n",
        "The output DataFrame will contain **ALL columns from your input CSV** plus new evaluation columns:\n",
        "\n",
        "**Original columns preserved:**\n",
        "- `question_id`, `question_text`, `document_idx`, `embedding_method`, etc.\n",
        "\n",
        "**New columns added:**\n",
        "- `question` - The actual question text from qa_df\n",
        "- `ground_truth` - The correct answer\n",
        "- `predicted_answer` - The LLM's answer\n",
        "- `num_docs` - Number of documents retrieved\n",
        "- `exact_match` - Boolean: exact match after normalization\n",
        "- `contains_answer` - Boolean: prediction contains ground truth\n",
        "- `f1_score` - Token-level F1 score\n",
        "- `cosine_similarity`, `euclidean_distance`, etc. (if embedding metrics enabled)\n",
        "- `bertscore_precision`, `bertscore_recall`, `bertscore_f1` (if BERTScore enabled)\n",
        "\n",
        "This makes it easy to analyze results by embedding method, k value, or any other column in your original data!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline: Process Multiple CSV Files\n",
        "\n",
        "Process all CSV files in a folder automatically!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def process_folder_pipeline(\n",
        "    input_folder: str,\n",
        "    output_folder: str,\n",
        "    qa_df: pd.DataFrame,\n",
        "    doc_df: pd.DataFrame,\n",
        "    question_id_col: str = 'question_id',\n",
        "    doc_ids_col: str = 'document_idx',\n",
        "    use_embedding_metrics: bool = True,\n",
        "    use_bertscore: bool = False,\n",
        "    verbose: bool = False,\n",
        "    file_pattern: str = '*.csv'\n",
        "):\n",
        "    \"\"\"\n",
        "    Process all CSV files in a folder with batch RAG evaluation.\n",
        "    \n",
        "    Args:\n",
        "        input_folder: Path to folder containing input CSV files\n",
        "        output_folder: Path to folder where results will be saved\n",
        "        qa_df: DataFrame with questions and answers\n",
        "        doc_df: DataFrame with documents\n",
        "        question_id_col: Column name for question IDs\n",
        "        doc_ids_col: Column name for document IDs\n",
        "        use_embedding_metrics: Whether to compute embedding metrics\n",
        "        use_bertscore: Whether to compute BERTScore\n",
        "        verbose: Whether to show detailed output per question\n",
        "        file_pattern: Glob pattern for files to process (default: '*.csv')\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with processing summary\n",
        "    \"\"\"\n",
        "    # Create output folder if it doesn't exist\n",
        "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Find all CSV files in input folder\n",
        "    input_path = Path(input_folder)\n",
        "    csv_files = list(input_path.glob(file_pattern))\n",
        "    \n",
        "    if not csv_files:\n",
        "        print(f\"‚ö†Ô∏è  No files matching '{file_pattern}' found in {input_folder}\")\n",
        "        return None\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(f\"üöÄ RAG EVALUATION PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Input folder: {input_folder}\")\n",
        "    print(f\"Output folder: {output_folder}\")\n",
        "    print(f\"Files to process: {len(csv_files)}\")\n",
        "    print(f\"Embedding metrics: {'Enabled' if use_embedding_metrics else 'Disabled'}\")\n",
        "    print(f\"BERTScore: {'Enabled' if use_bertscore else 'Disabled'}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    results_summary = []\n",
        "    \n",
        "    for file_idx, input_file in enumerate(csv_files, 1):\n",
        "        print(f\"\\nüìÑ [{file_idx}/{len(csv_files)}] Processing: {input_file.name}\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        try:\n",
        "            # Load input CSV\n",
        "            retrieval_results = pd.read_csv(input_file)\n",
        "            print(f\"   Loaded {len(retrieval_results)} rows\")\n",
        "            \n",
        "            # Run batch evaluation\n",
        "            results_df = evaluate_rag_batch(\n",
        "                retrieval_results_df=retrieval_results,\n",
        "                qa_df=qa_df,\n",
        "                doc_df=doc_df,\n",
        "                question_id_col=question_id_col,\n",
        "                doc_ids_col=doc_ids_col,\n",
        "                use_embedding_metrics=use_embedding_metrics,\n",
        "                use_bertscore=use_bertscore,\n",
        "                verbose=verbose,\n",
        "                show_progress=True\n",
        "            )\n",
        "            \n",
        "            # Generate output filename\n",
        "            output_filename = f\"results_{input_file.stem}.csv\"\n",
        "            output_path = Path(output_folder) / output_filename\n",
        "            \n",
        "            # Save results\n",
        "            results_df.to_csv(output_path, index=False)\n",
        "            print(f\"   ‚úì Saved {len(results_df)} results to: {output_filename}\")\n",
        "            \n",
        "            # Compute summary statistics\n",
        "            summary = {\n",
        "                'input_file': input_file.name,\n",
        "                'output_file': output_filename,\n",
        "                'total_rows': len(retrieval_results),\n",
        "                'successful_evals': len(results_df),\n",
        "                'exact_match_rate': results_df['exact_match'].mean(),\n",
        "                'mean_f1_score': results_df['f1_score'].mean()\n",
        "            }\n",
        "            \n",
        "            if 'bertscore_f1' in results_df.columns:\n",
        "                summary['mean_bertscore_f1'] = results_df['bertscore_f1'].mean()\n",
        "            if 'cosine_similarity' in results_df.columns:\n",
        "                summary['mean_cosine_similarity'] = results_df['cosine_similarity'].mean()\n",
        "            \n",
        "            results_summary.append(summary)\n",
        "            \n",
        "            # Show quick stats\n",
        "            print(f\"   üìä Quick Stats:\")\n",
        "            print(f\"      Exact Match Rate: {summary['exact_match_rate']:.2%}\")\n",
        "            print(f\"      Mean F1 Score: {summary['mean_f1_score']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error processing {input_file.name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            results_summary.append({\n",
        "                'input_file': input_file.name,\n",
        "                'output_file': None,\n",
        "                'error': str(e)\n",
        "            })\n",
        "    \n",
        "    # Print final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üìä PIPELINE SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    successful = sum(1 for s in results_summary if 'error' not in s)\n",
        "    print(f\"Files processed: {len(csv_files)}\")\n",
        "    print(f\"Successful: {successful}\")\n",
        "    print(f\"Failed: {len(csv_files) - successful}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Save summary to CSV\n",
        "    summary_df = pd.DataFrame(results_summary)\n",
        "    summary_path = Path(output_folder) / \"pipeline_summary.csv\"\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    print(f\"\\n‚úì Pipeline summary saved to: {summary_path}\")\n",
        "    \n",
        "    return summary_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage: Process All Files in a Folder\n",
        "\n",
        "**What it does:**\n",
        "1. Finds all CSV files in your input folder\n",
        "2. Processes each file with batch evaluation\n",
        "3. Saves results to output folder with naming: `results_<original_filename>.csv`\n",
        "4. Creates a summary CSV with statistics for all files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üöÄ RAG EVALUATION PIPELINE\n",
            "================================================================================\n",
            "Input folder: grid_results\n",
            "Output folder: result\n",
            "Files to process: 5\n",
            "Embedding metrics: Enabled\n",
            "BERTScore: Disabled\n",
            "================================================================================\n",
            "\n",
            "üìÑ [1/5] Processing: retrieval_results_cutting_plane.csv\n",
            "--------------------------------------------------------------------------------\n",
            "   Loaded 360 rows\n",
            "Progress: 10/360 questions evaluated...\n",
            "Progress: 20/360 questions evaluated...\n",
            "Progress: 30/360 questions evaluated...\n",
            "Progress: 40/360 questions evaluated...\n",
            "Progress: 50/360 questions evaluated...\n",
            "Progress: 60/360 questions evaluated...\n",
            "Progress: 70/360 questions evaluated...\n",
            "Progress: 80/360 questions evaluated...\n",
            "Progress: 90/360 questions evaluated...\n",
            "Progress: 100/360 questions evaluated...\n",
            "Progress: 110/360 questions evaluated...\n",
            "Progress: 120/360 questions evaluated...\n",
            "Progress: 130/360 questions evaluated...\n",
            "Progress: 140/360 questions evaluated...\n",
            "Progress: 150/360 questions evaluated...\n",
            "Progress: 160/360 questions evaluated...\n",
            "Progress: 170/360 questions evaluated...\n",
            "Progress: 180/360 questions evaluated...\n",
            "Progress: 190/360 questions evaluated...\n",
            "Progress: 200/360 questions evaluated...\n",
            "Progress: 210/360 questions evaluated...\n",
            "Progress: 220/360 questions evaluated...\n",
            "Progress: 230/360 questions evaluated...\n",
            "Progress: 240/360 questions evaluated...\n",
            "Progress: 250/360 questions evaluated...\n",
            "Progress: 260/360 questions evaluated...\n",
            "Progress: 270/360 questions evaluated...\n",
            "Progress: 280/360 questions evaluated...\n",
            "Progress: 290/360 questions evaluated...\n",
            "Progress: 300/360 questions evaluated...\n",
            "Progress: 310/360 questions evaluated...\n",
            "Progress: 320/360 questions evaluated...\n",
            "Progress: 330/360 questions evaluated...\n",
            "Progress: 340/360 questions evaluated...\n",
            "Progress: 350/360 questions evaluated...\n",
            "Progress: 360/360 questions evaluated...\n",
            "\n",
            "‚úì Completed! Evaluated 360 questions.\n",
            "   ‚úì Saved 360 results to: results_retrieval_results_cutting_plane.csv\n",
            "   üìä Quick Stats:\n",
            "      Exact Match Rate: 65.00%\n",
            "      Mean F1 Score: 0.705\n",
            "\n",
            "üìÑ [2/5] Processing: retrieval_results_robust_l2.csv\n",
            "--------------------------------------------------------------------------------\n",
            "   Loaded 240 rows\n",
            "Progress: 10/240 questions evaluated...\n",
            "Progress: 20/240 questions evaluated...\n",
            "Progress: 30/240 questions evaluated...\n",
            "Progress: 40/240 questions evaluated...\n",
            "Progress: 50/240 questions evaluated...\n",
            "Progress: 60/240 questions evaluated...\n",
            "Progress: 70/240 questions evaluated...\n",
            "Progress: 80/240 questions evaluated...\n",
            "Progress: 90/240 questions evaluated...\n",
            "Progress: 100/240 questions evaluated...\n",
            "Progress: 110/240 questions evaluated...\n",
            "Progress: 120/240 questions evaluated...\n",
            "Progress: 130/240 questions evaluated...\n",
            "Progress: 140/240 questions evaluated...\n",
            "Progress: 150/240 questions evaluated...\n",
            "Progress: 160/240 questions evaluated...\n",
            "Progress: 170/240 questions evaluated...\n",
            "Progress: 180/240 questions evaluated...\n",
            "Progress: 190/240 questions evaluated...\n",
            "Progress: 200/240 questions evaluated...\n",
            "Progress: 210/240 questions evaluated...\n",
            "Progress: 220/240 questions evaluated...\n",
            "Progress: 230/240 questions evaluated...\n",
            "Progress: 240/240 questions evaluated...\n",
            "\n",
            "‚úì Completed! Evaluated 240 questions.\n",
            "   ‚úì Saved 240 results to: results_retrieval_results_robust_l2.csv\n",
            "   üìä Quick Stats:\n",
            "      Exact Match Rate: 70.00%\n",
            "      Mean F1 Score: 0.751\n",
            "\n",
            "üìÑ [3/5] Processing: retrieval_results_robust_l1.csv\n",
            "--------------------------------------------------------------------------------\n",
            "   Loaded 240 rows\n",
            "Progress: 10/240 questions evaluated...\n",
            "Progress: 20/240 questions evaluated...\n",
            "Progress: 30/240 questions evaluated...\n",
            "Progress: 40/240 questions evaluated...\n",
            "Progress: 50/240 questions evaluated...\n",
            "Progress: 60/240 questions evaluated...\n",
            "Progress: 70/240 questions evaluated...\n",
            "Progress: 80/240 questions evaluated...\n",
            "Progress: 90/240 questions evaluated...\n",
            "Progress: 100/240 questions evaluated...\n",
            "Progress: 110/240 questions evaluated...\n",
            "Progress: 120/240 questions evaluated...\n",
            "Progress: 130/240 questions evaluated...\n",
            "Progress: 140/240 questions evaluated...\n",
            "Progress: 150/240 questions evaluated...\n",
            "Progress: 160/240 questions evaluated...\n",
            "Progress: 170/240 questions evaluated...\n",
            "Progress: 180/240 questions evaluated...\n",
            "Progress: 190/240 questions evaluated...\n",
            "Progress: 200/240 questions evaluated...\n",
            "Progress: 210/240 questions evaluated...\n",
            "Progress: 220/240 questions evaluated...\n",
            "Progress: 230/240 questions evaluated...\n",
            "Progress: 240/240 questions evaluated...\n",
            "\n",
            "‚úì Completed! Evaluated 240 questions.\n",
            "   ‚úì Saved 240 results to: results_retrieval_results_robust_l1.csv\n",
            "   üìä Quick Stats:\n",
            "      Exact Match Rate: 69.17%\n",
            "      Mean F1 Score: 0.736\n",
            "\n",
            "üìÑ [4/5] Processing: retrieval_results_robust_linf.csv\n",
            "--------------------------------------------------------------------------------\n",
            "   Loaded 240 rows\n",
            "Progress: 10/240 questions evaluated...\n",
            "Progress: 20/240 questions evaluated...\n",
            "Progress: 30/240 questions evaluated...\n",
            "Progress: 40/240 questions evaluated...\n",
            "Progress: 50/240 questions evaluated...\n",
            "Progress: 60/240 questions evaluated...\n",
            "Progress: 70/240 questions evaluated...\n",
            "Progress: 80/240 questions evaluated...\n",
            "Progress: 90/240 questions evaluated...\n",
            "Progress: 100/240 questions evaluated...\n",
            "Progress: 110/240 questions evaluated...\n",
            "Progress: 120/240 questions evaluated...\n",
            "Progress: 130/240 questions evaluated...\n",
            "Progress: 140/240 questions evaluated...\n",
            "Progress: 150/240 questions evaluated...\n",
            "Progress: 160/240 questions evaluated...\n",
            "Progress: 170/240 questions evaluated...\n",
            "Progress: 180/240 questions evaluated...\n",
            "Progress: 190/240 questions evaluated...\n",
            "Progress: 200/240 questions evaluated...\n",
            "Progress: 210/240 questions evaluated...\n",
            "Progress: 220/240 questions evaluated...\n",
            "Progress: 230/240 questions evaluated...\n",
            "Progress: 240/240 questions evaluated...\n",
            "\n",
            "‚úì Completed! Evaluated 240 questions.\n",
            "   ‚úì Saved 240 results to: results_retrieval_results_robust_linf.csv\n",
            "   üìä Quick Stats:\n",
            "      Exact Match Rate: 65.00%\n",
            "      Mean F1 Score: 0.707\n",
            "\n",
            "üìÑ [5/5] Processing: retrieval_results_topk.csv\n",
            "--------------------------------------------------------------------------------\n",
            "   Loaded 100 rows\n",
            "Progress: 10/100 questions evaluated...\n",
            "Progress: 20/100 questions evaluated...\n",
            "Progress: 30/100 questions evaluated...\n",
            "Progress: 40/100 questions evaluated...\n",
            "Progress: 50/100 questions evaluated...\n",
            "Progress: 60/100 questions evaluated...\n",
            "Progress: 70/100 questions evaluated...\n",
            "Progress: 80/100 questions evaluated...\n",
            "Progress: 90/100 questions evaluated...\n",
            "Progress: 100/100 questions evaluated...\n",
            "\n",
            "‚úì Completed! Evaluated 100 questions.\n",
            "   ‚úì Saved 100 results to: results_retrieval_results_topk.csv\n",
            "   üìä Quick Stats:\n",
            "      Exact Match Rate: 77.00%\n",
            "      Mean F1 Score: 0.811\n",
            "\n",
            "================================================================================\n",
            "üìä PIPELINE SUMMARY\n",
            "================================================================================\n",
            "Files processed: 5\n",
            "Successful: 5\n",
            "Failed: 0\n",
            "================================================================================\n",
            "\n",
            "‚úì Pipeline summary saved to: result/pipeline_summary.csv\n",
            "\n",
            "Processing Summary:\n",
            "                            input_file  \\\n",
            "0  retrieval_results_cutting_plane.csv   \n",
            "1      retrieval_results_robust_l2.csv   \n",
            "2      retrieval_results_robust_l1.csv   \n",
            "3    retrieval_results_robust_linf.csv   \n",
            "4           retrieval_results_topk.csv   \n",
            "\n",
            "                                   output_file  total_rows  successful_evals  \\\n",
            "0  results_retrieval_results_cutting_plane.csv         360               360   \n",
            "1      results_retrieval_results_robust_l2.csv         240               240   \n",
            "2      results_retrieval_results_robust_l1.csv         240               240   \n",
            "3    results_retrieval_results_robust_linf.csv         240               240   \n",
            "4           results_retrieval_results_topk.csv         100               100   \n",
            "\n",
            "   exact_match_rate  mean_f1_score  mean_cosine_similarity  \n",
            "0          0.650000       0.704785                0.846673  \n",
            "1          0.700000       0.750935                0.874968  \n",
            "2          0.691667       0.735899                0.870531  \n",
            "3          0.650000       0.707083                0.855062  \n",
            "4          0.770000       0.810908                0.883508  \n"
          ]
        }
      ],
      "source": [
        "# Run the pipeline on all CSV files in a folder\n",
        "summary_df = process_folder_pipeline(\n",
        "    input_folder='grid_results',        # Folder with your retrieval results CSVs\n",
        "    output_folder='result',           # Where to save results\n",
        "    qa_df=qa_df,\n",
        "    doc_df=doc_df,\n",
        "    question_id_col='question_id',\n",
        "    doc_ids_col='document_id',\n",
        "    use_embedding_metrics=True,          # Set to False for faster processing\n",
        "    use_bertscore=False,                 # Set to True to enable (much slower)\n",
        "    verbose=False,                       # Set to True to see details per question\n",
        "    file_pattern='*.csv'                 # Process all CSV files\n",
        ")\n",
        "\n",
        "# View the summary\n",
        "print(\"\\nProcessing Summary:\")\n",
        "print(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Expected CSV Format\n",
        "\n",
        "Your CSV file should have at least these two columns:\n",
        "\n",
        "| question_id | doc_ids |\n",
        "|-------------|---------|\n",
        "| 0 | [1, 5, 10] |\n",
        "| 2 | [3, 7] |\n",
        "| 4 | [2, 4, 8, 12] |\n",
        "\n",
        "**Supported doc_ids formats:**\n",
        "- `[1, 5, 10]` - List format (with brackets)\n",
        "- `1, 5, 10` - Comma-separated\n",
        "- `1 5 10` - Space-separated\n",
        "- `1` - Single document ID\n",
        "\n",
        "**Column names can be different** - just specify them in the function call!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Load Your Retrieval Results\n",
        "\n",
        "**Option 1: If you have a CSV file**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 400 retrieval results\n",
            "\n",
            "First few rows:\n",
            "   question_id                                      question_text  \\\n",
            "0          646  Is the smallest penguin species the Little Blu...   \n",
            "1          646  Is the smallest penguin species the Little Blu...   \n",
            "2          646  Is the smallest penguin species the Little Blu...   \n",
            "3          646  Is the smallest penguin species the Little Blu...   \n",
            "4          873  What does a citizen use to propose changes to ...   \n",
            "\n",
            "              document_idx            embedding_method  \n",
            "0  953,1182,1192,1204,2595           bert-base-uncased  \n",
            "1      952,953,967,975,979  multi-qa-mpnet-base-dot-v1  \n",
            "2  950,2467,2471,2596,2642     hkunlp-instructor-large  \n",
            "3      951,952,953,979,981        intfloat-e5-small-v2  \n",
            "4      25,656,657,658,2141           bert-base-uncased  \n",
            "\n",
            "Column names: ['question_id', 'question_text', 'document_idx', 'embedding_method']\n"
          ]
        }
      ],
      "source": [
        "# Load your retrieval results CSV\n",
        "# Replace 'your_file.csv' with your actual file path\n",
        "\n",
        "grid_results_dir = 'grid_results'\n",
        "\n",
        "# Collect all CSV files in the grid_results folder (skip hidden files)\n",
        "grid_result_files = [\n",
        "    os.path.join(grid_results_dir, f)\n",
        "    for f in os.listdir(grid_results_dir)\n",
        "    if f.endswith('.csv') and not f.startswith('.')\n",
        "]\n",
        "\n",
        "for f in grid_result_files:\n",
        "    print(\"-\", f)\n",
        "\n",
        "# Optionally: demonstrate reading the first one\n",
        "if grid_result_files:\n",
        "    retrieval_results = pd.read_csv(grid_result_files[0])  # For downstream cells - load the first by default\n",
        "    print(f\"\\nLoaded {len(retrieval_results)} retrieval results from {grid_result_files[0]}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(retrieval_results.head())\n",
        "    print(\"\\nColumn names:\", retrieval_results.columns.tolist())\n",
        "else:\n",
        "    retrieval_results = None\n",
        "    print(\"No retrieval results files found in grid_results/\")\n",
        "\n",
        "# retrieval_results = pd.read_csv('grid_results/retrieval_results_heuristic_k5.csv')\n",
        "\n",
        "# Preview the data\n",
        "print(f\"Loaded {len(retrieval_results)} retrieval results\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(retrieval_results.head())\n",
        "print(\"\\nColumn names:\", retrieval_results.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Run Batch Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 10/400 questions evaluated...\n",
            "Progress: 20/400 questions evaluated...\n",
            "Progress: 30/400 questions evaluated...\n",
            "Progress: 40/400 questions evaluated...\n",
            "Progress: 50/400 questions evaluated...\n",
            "Progress: 60/400 questions evaluated...\n",
            "Progress: 70/400 questions evaluated...\n",
            "Progress: 80/400 questions evaluated...\n",
            "Progress: 90/400 questions evaluated...\n",
            "Progress: 100/400 questions evaluated...\n",
            "Progress: 110/400 questions evaluated...\n",
            "Progress: 120/400 questions evaluated...\n",
            "Progress: 130/400 questions evaluated...\n",
            "Progress: 140/400 questions evaluated...\n",
            "Progress: 150/400 questions evaluated...\n",
            "Progress: 160/400 questions evaluated...\n",
            "Progress: 170/400 questions evaluated...\n",
            "Progress: 180/400 questions evaluated...\n",
            "Progress: 190/400 questions evaluated...\n",
            "Progress: 200/400 questions evaluated...\n",
            "Progress: 210/400 questions evaluated...\n",
            "Progress: 220/400 questions evaluated...\n",
            "Progress: 230/400 questions evaluated...\n",
            "Progress: 240/400 questions evaluated...\n",
            "Progress: 250/400 questions evaluated...\n",
            "Progress: 260/400 questions evaluated...\n",
            "Progress: 270/400 questions evaluated...\n",
            "Progress: 280/400 questions evaluated...\n",
            "Progress: 290/400 questions evaluated...\n",
            "Progress: 300/400 questions evaluated...\n",
            "Progress: 310/400 questions evaluated...\n",
            "Progress: 320/400 questions evaluated...\n",
            "Progress: 330/400 questions evaluated...\n",
            "Progress: 340/400 questions evaluated...\n",
            "Progress: 350/400 questions evaluated...\n",
            "Warning: Document ID 694 not found\n",
            "Progress: 360/400 questions evaluated...\n",
            "Progress: 370/400 questions evaluated...\n",
            "Progress: 380/400 questions evaluated...\n",
            "Progress: 390/400 questions evaluated...\n",
            "Progress: 400/400 questions evaluated...\n",
            "\n",
            "‚úì Completed! Evaluated 400 questions.\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS SUMMARY\n",
            "================================================================================\n",
            "Total questions evaluated: 400\n",
            "\n",
            "Average scores:\n",
            "  Exact Match: 0.233\n",
            "  F1 Score: 0.288\n",
            "  Cosine Similarity: 0.670\n",
            "  BERTScore F1: 0.758\n"
          ]
        }
      ],
      "source": [
        "# Run batch evaluation\n",
        "# The 'question_id' column in your CSV contains the INDEX to look up in qa_df\n",
        "# For example: question_id=646 will look up qa_df.iloc[646]\n",
        "results_df = evaluate_rag_batch(\n",
        "    retrieval_results_df=retrieval_results,\n",
        "    qa_df=qa_df,\n",
        "    doc_df=doc_df,\n",
        "    question_id_col='question_id',   # Column in CSV containing the question index\n",
        "    doc_ids_col='document_idx',      # Column containing the document IDs\n",
        "    use_embedding_metrics=True,      # Set to False to skip embedding metrics\n",
        "    use_bertscore=True,              # Set to True to enable BERTScore (slower)\n",
        "    verbose=False,                   # Set to True to see details for each question\n",
        "    show_progress=True               # Shows progress updates\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total questions evaluated: {len(results_df)}\")\n",
        "print(f\"\\nAverage scores:\")\n",
        "print(f\"  Exact Match: {results_df['exact_match'].mean():.3f}\")\n",
        "print(f\"  F1 Score: {results_df['f1_score'].mean():.3f}\")\n",
        "if 'cosine_similarity' in results_df.columns:\n",
        "    print(f\"  Cosine Similarity: {results_df['cosine_similarity'].mean():.3f}\")\n",
        "if 'bertscore_f1' in results_df.columns:\n",
        "    print(f\"  BERTScore F1: {results_df['bertscore_f1'].mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4 View and Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_idx</th>\n",
              "      <th>question</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>predicted_answer</th>\n",
              "      <th>doc_ids</th>\n",
              "      <th>num_docs</th>\n",
              "      <th>exact_match</th>\n",
              "      <th>contains_answer</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>cosine_similarity</th>\n",
              "      <th>euclidean_distance</th>\n",
              "      <th>dot_product_similarity</th>\n",
              "      <th>manhattan_distance</th>\n",
              "      <th>bertscore_precision</th>\n",
              "      <th>bertscore_recall</th>\n",
              "      <th>bertscore_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>646</td>\n",
              "      <td>Is it not even known whether the gigantic pala...</td>\n",
              "      <td>It is not even known.</td>\n",
              "      <td>No.</td>\n",
              "      <td>[953, 1182, 1192, 1204, 2595]</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.532425</td>\n",
              "      <td>0.967032</td>\n",
              "      <td>0.532425</td>\n",
              "      <td>21.478205</td>\n",
              "      <td>0.582180</td>\n",
              "      <td>0.639000</td>\n",
              "      <td>0.609268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>646</td>\n",
              "      <td>Is it not even known whether the gigantic pala...</td>\n",
              "      <td>It is not even known.</td>\n",
              "      <td>It is not even known.</td>\n",
              "      <td>[952, 953, 967, 975, 979]</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>646</td>\n",
              "      <td>Is it not even known whether the gigantic pala...</td>\n",
              "      <td>It is not even known.</td>\n",
              "      <td>No.</td>\n",
              "      <td>[950, 2467, 2471, 2596, 2642]</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.532425</td>\n",
              "      <td>0.967032</td>\n",
              "      <td>0.532425</td>\n",
              "      <td>21.478205</td>\n",
              "      <td>0.582180</td>\n",
              "      <td>0.639000</td>\n",
              "      <td>0.609268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>646</td>\n",
              "      <td>Is it not even known whether the gigantic pala...</td>\n",
              "      <td>It is not even known.</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>[951, 952, 953, 979, 981]</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.539930</td>\n",
              "      <td>0.959239</td>\n",
              "      <td>0.539930</td>\n",
              "      <td>21.276486</td>\n",
              "      <td>0.591416</td>\n",
              "      <td>0.619107</td>\n",
              "      <td>0.604945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>873</td>\n",
              "      <td>What religions are found in Uruguay?</td>\n",
              "      <td>Roman Catholic, Protestant, Jewish, and nonpro...</td>\n",
              "      <td>None.</td>\n",
              "      <td>[25, 656, 657, 658, 2141]</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.442526</td>\n",
              "      <td>1.055911</td>\n",
              "      <td>0.442526</td>\n",
              "      <td>23.180184</td>\n",
              "      <td>0.746263</td>\n",
              "      <td>0.617503</td>\n",
              "      <td>0.675805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>794</td>\n",
              "      <td>What shape are the eggs of the larest species ...</td>\n",
              "      <td>Spherical</td>\n",
              "      <td>Not mentioned.</td>\n",
              "      <td>[2684, 2688, 2692, 2693, 2698]</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301685</td>\n",
              "      <td>1.181791</td>\n",
              "      <td>0.301685</td>\n",
              "      <td>25.363775</td>\n",
              "      <td>0.626011</td>\n",
              "      <td>0.655157</td>\n",
              "      <td>0.640253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>332</td>\n",
              "      <td>Did the Department of the Interior not charge ...</td>\n",
              "      <td>yes</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>[525, 699, 706, 1396, 2226]</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.965360</td>\n",
              "      <td>0.263210</td>\n",
              "      <td>0.965360</td>\n",
              "      <td>5.764243</td>\n",
              "      <td>0.815256</td>\n",
              "      <td>0.889402</td>\n",
              "      <td>0.850716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>332</td>\n",
              "      <td>Did the Department of the Interior not charge ...</td>\n",
              "      <td>yes</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>[147, 525, 567, 1967, 2999]</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.965360</td>\n",
              "      <td>0.263210</td>\n",
              "      <td>0.965360</td>\n",
              "      <td>5.764243</td>\n",
              "      <td>0.815256</td>\n",
              "      <td>0.889402</td>\n",
              "      <td>0.850716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>332</td>\n",
              "      <td>Did the Department of the Interior not charge ...</td>\n",
              "      <td>yes</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>[117, 289, 567, 2718, 2903]</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.965360</td>\n",
              "      <td>0.263210</td>\n",
              "      <td>0.965360</td>\n",
              "      <td>5.764243</td>\n",
              "      <td>0.815256</td>\n",
              "      <td>0.889402</td>\n",
              "      <td>0.850716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>332</td>\n",
              "      <td>Did the Department of the Interior not charge ...</td>\n",
              "      <td>yes</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>[147, 188, 262, 525, 3002]</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.965360</td>\n",
              "      <td>0.263210</td>\n",
              "      <td>0.965360</td>\n",
              "      <td>5.764243</td>\n",
              "      <td>0.815256</td>\n",
              "      <td>0.889402</td>\n",
              "      <td>0.850716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows √ó 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     question_idx                                           question  \\\n",
              "0             646  Is it not even known whether the gigantic pala...   \n",
              "1             646  Is it not even known whether the gigantic pala...   \n",
              "2             646  Is it not even known whether the gigantic pala...   \n",
              "3             646  Is it not even known whether the gigantic pala...   \n",
              "4             873               What religions are found in Uruguay?   \n",
              "..            ...                                                ...   \n",
              "395           794  What shape are the eggs of the larest species ...   \n",
              "396           332  Did the Department of the Interior not charge ...   \n",
              "397           332  Did the Department of the Interior not charge ...   \n",
              "398           332  Did the Department of the Interior not charge ...   \n",
              "399           332  Did the Department of the Interior not charge ...   \n",
              "\n",
              "                                          ground_truth       predicted_answer  \\\n",
              "0                                It is not even known.                    No.   \n",
              "1                                It is not even known.  It is not even known.   \n",
              "2                                It is not even known.                    No.   \n",
              "3                                It is not even known.                   Yes.   \n",
              "4    Roman Catholic, Protestant, Jewish, and nonpro...                  None.   \n",
              "..                                                 ...                    ...   \n",
              "395                                          Spherical         Not mentioned.   \n",
              "396                                                yes                   Yes.   \n",
              "397                                                yes                   Yes.   \n",
              "398                                                yes                   Yes.   \n",
              "399                                                yes                   Yes.   \n",
              "\n",
              "                            doc_ids  num_docs  exact_match  contains_answer  \\\n",
              "0     [953, 1182, 1192, 1204, 2595]         5        False            False   \n",
              "1         [952, 953, 967, 975, 979]         5         True             True   \n",
              "2     [950, 2467, 2471, 2596, 2642]         5        False            False   \n",
              "3         [951, 952, 953, 979, 981]         5        False            False   \n",
              "4         [25, 656, 657, 658, 2141]         5        False            False   \n",
              "..                              ...       ...          ...              ...   \n",
              "395  [2684, 2688, 2692, 2693, 2698]         5        False            False   \n",
              "396     [525, 699, 706, 1396, 2226]         5         True             True   \n",
              "397     [147, 525, 567, 1967, 2999]         5         True             True   \n",
              "398     [117, 289, 567, 2718, 2903]         5         True             True   \n",
              "399      [147, 188, 262, 525, 3002]         5         True             True   \n",
              "\n",
              "     f1_score  cosine_similarity  euclidean_distance  dot_product_similarity  \\\n",
              "0         0.0           0.532425            0.967032                0.532425   \n",
              "1         1.0           1.000000            0.000000                1.000000   \n",
              "2         0.0           0.532425            0.967032                0.532425   \n",
              "3         0.0           0.539930            0.959239                0.539930   \n",
              "4         0.0           0.442526            1.055911                0.442526   \n",
              "..        ...                ...                 ...                     ...   \n",
              "395       0.0           0.301685            1.181791                0.301685   \n",
              "396       1.0           0.965360            0.263210                0.965360   \n",
              "397       1.0           0.965360            0.263210                0.965360   \n",
              "398       1.0           0.965360            0.263210                0.965360   \n",
              "399       1.0           0.965360            0.263210                0.965360   \n",
              "\n",
              "     manhattan_distance  bertscore_precision  bertscore_recall  bertscore_f1  \n",
              "0             21.478205             0.582180          0.639000      0.609268  \n",
              "1              0.000000             1.000000          1.000000      1.000000  \n",
              "2             21.478205             0.582180          0.639000      0.609268  \n",
              "3             21.276486             0.591416          0.619107      0.604945  \n",
              "4             23.180184             0.746263          0.617503      0.675805  \n",
              "..                  ...                  ...               ...           ...  \n",
              "395           25.363775             0.626011          0.655157      0.640253  \n",
              "396            5.764243             0.815256          0.889402      0.850716  \n",
              "397            5.764243             0.815256          0.889402      0.850716  \n",
              "398            5.764243             0.815256          0.889402      0.850716  \n",
              "399            5.764243             0.815256          0.889402      0.850716  \n",
              "\n",
              "[400 rows x 16 columns]"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample results (first 5):\n",
            "   question_idx                                           question  \\\n",
            "0           646  Is it not even known whether the gigantic pala...   \n",
            "1           646  Is it not even known whether the gigantic pala...   \n",
            "2           646  Is it not even known whether the gigantic pala...   \n",
            "3           646  Is it not even known whether the gigantic pala...   \n",
            "4           873               What religions are found in Uruguay?   \n",
            "\n",
            "                                        ground_truth       predicted_answer  \\\n",
            "0                              It is not even known.                    No.   \n",
            "1                              It is not even known.  It is not even known.   \n",
            "2                              It is not even known.                    No.   \n",
            "3                              It is not even known.                   Yes.   \n",
            "4  Roman Catholic, Protestant, Jewish, and nonpro...                  None.   \n",
            "\n",
            "   exact_match  f1_score  bertscore_precision  \n",
            "0        False       0.0             0.582180  \n",
            "1         True       1.0             1.000000  \n",
            "2        False       0.0             0.582180  \n",
            "3        False       0.0             0.591416  \n",
            "4        False       0.0             0.746263  \n",
            "\n",
            "\n",
            "All available columns:\n",
            "['question_idx', 'question', 'ground_truth', 'predicted_answer', 'doc_ids', 'num_docs', 'exact_match', 'contains_answer', 'f1_score', 'cosine_similarity', 'euclidean_distance', 'dot_product_similarity', 'manhattan_distance', 'bertscore_precision', 'bertscore_recall', 'bertscore_f1']\n"
          ]
        }
      ],
      "source": [
        "# View sample results\n",
        "print(\"Sample results (first 5):\")\n",
        "print(results_df[['question_idx', 'question', 'ground_truth', 'predicted_answer', \n",
        "                  'exact_match', 'f1_score', 'bertscore_precision']].head())\n",
        "\n",
        "# View all columns\n",
        "print(\"\\n\\nAll available columns:\")\n",
        "print(results_df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Results saved to: rag_evaluation_results.csv\n",
            "‚úì Summary saved to: rag_evaluation_summary.csv\n"
          ]
        }
      ],
      "source": [
        "# Save results to CSV\n",
        "output_file = 'rag_evaluation_results.csv'\n",
        "results_df.to_csv(output_file, index=False)\n",
        "print(f\"‚úì Results saved to: {output_file}\")\n",
        "\n",
        "# You can also save just the summary metrics\n",
        "summary_file = 'rag_evaluation_summary.csv'\n",
        "summary_cols = ['question_idx', 'exact_match', 'f1_score']\n",
        "if 'cosine_similarity' in results_df.columns:\n",
        "    summary_cols.append('cosine_similarity')\n",
        "if 'bertscore_f1' in results_df.columns:\n",
        "    summary_cols.append('bertscore_f1')\n",
        "summary_df = results_df[summary_cols].copy()\n",
        "summary_df.to_csv(summary_file, index=False)\n",
        "print(f\"‚úì Summary saved to: {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.5 Analyze Results\n",
        "\n",
        "Once you have the results, you can analyze them in various ways:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perfect matches: 93 / 400 (23.2%)\n",
            "\n",
            "High F1 (>0.5) but not exact match: 7\n",
            "Examples:\n",
            "  Q649: GT='At least one giant penguin.' | Pred='At least one giant penguin occurred in this region.' | F1=0.714\n",
            "  Q895: GT='history and political science' | Pred='Political Science' | F1=0.667\n",
            "  Q896: GT='German began unrestricted submarine warfare' | Pred='Germany's resumption of unrestricted submarine warfare.' | F1=0.545\n",
            "\n",
            "Worst performing questions (by F1):\n",
            "  Q646: F1=0.000 | GT='It is not even known.' | Pred='No.'\n",
            "  Q646: F1=0.000 | GT='It is not even known.' | Pred='No.'\n",
            "  Q646: F1=0.000 | GT='It is not even known.' | Pred='Yes.'\n",
            "  Q873: F1=0.000 | GT='Roman Catholic, Protestant, Jewish, and nonprofessing.' | Pred='None.'\n",
            "  Q873: F1=0.000 | GT='Roman Catholic, Protestant, Jewish, and nonprofessing.' | Pred='None (answer not found in provided documents)'\n",
            "\n",
            "\n",
            "Metric Correlations:\n",
            "                   f1_score  bertscore_f1  cosine_similarity\n",
            "f1_score           1.000000      0.666879           0.575936\n",
            "bertscore_f1       0.666879      1.000000           0.851596\n",
            "cosine_similarity  0.575936      0.851596           1.000000\n"
          ]
        }
      ],
      "source": [
        "# 1. Find questions with perfect exact match\n",
        "perfect_matches = results_df[results_df['exact_match'] == True]\n",
        "print(f\"Perfect matches: {len(perfect_matches)} / {len(results_df)} ({len(perfect_matches)/len(results_df)*100:.1f}%)\")\n",
        "\n",
        "# 2. Find questions with high F1 but not exact match\n",
        "high_f1_not_exact = results_df[(results_df['f1_score'] > 0.5) & (results_df['exact_match'] == False)]\n",
        "print(f\"\\nHigh F1 (>0.5) but not exact match: {len(high_f1_not_exact)}\")\n",
        "if len(high_f1_not_exact) > 0:\n",
        "    print(\"Examples:\")\n",
        "    for _, row in high_f1_not_exact.head(3).iterrows():\n",
        "        print(f\"  Q{row['question_idx']}: GT='{row['ground_truth']}' | Pred='{row['predicted_answer']}' | F1={row['f1_score']:.3f}\")\n",
        "\n",
        "# 3. Find worst performing questions\n",
        "worst = results_df.nsmallest(5, 'f1_score')\n",
        "print(f\"\\nWorst performing questions (by F1):\")\n",
        "for _, row in worst.iterrows():\n",
        "    print(f\"  Q{row['question_idx']}: F1={row['f1_score']:.3f} | GT='{row['ground_truth']}' | Pred='{row['predicted_answer']}'\")\n",
        "\n",
        "# 4. Correlation between metrics (if BERTScore was computed)\n",
        "if 'bertscore_f1' in results_df.columns:\n",
        "    corr = results_df[['f1_score', 'bertscore_f1', 'cosine_similarity']].corr()\n",
        "    print(\"\\n\\nMetric Correlations:\")\n",
        "    print(corr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
