{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222dc60b",
   "metadata": {},
   "source": [
    "# Heuristic Approach\n",
    "We first start by reconstructing a heuristic (i.e., Top-K Similarity Search) to approach the document retrieval problem in RAG. This serves as the **baseline optimization formulation**, which is equivalent to simply selecting the top-\\(k\\) most similar documents to a given query. It can be viewed as a **special case of a knapsack problem** where each document contributes a \"value\" equal to its cosine similarity score and all items have identical \"costs\" and a total capacity constraint k. The problem can be solved with the below formulation:\n",
    "\n",
    "\n",
    "**Sets & Index**\n",
    "- $I=\\{1,\\dots,n\\}$: index set of candidate documents (single query)\n",
    "\n",
    "**Parameters**\n",
    "- $q\\in\\mathbb{R}^d$: query embedding  \n",
    "- $\\mu_i\\in\\mathbb{R}^d$: embedding of document $i\\in I$  \n",
    "- $s_i := \\dfrac{q^\\top \\mu_i}{\\|q\\|_2\\,\\|\\mu_i\\|_2}$ (cosine similarity to the query);  \n",
    "  if embeddings are unit-normalized, $s_i = q^\\top \\mu_i$  \n",
    "- $k\\in\\{1,\\dots,n\\}$: exact number of documents to select (retrieval size)\n",
    "\n",
    "**Decision Variables**\n",
    "- $x_i\\in\\{0,1\\}$: equals 1 if document $i$ is selected; 0 otherwise\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x}\\quad \n",
    "& \\sum_{i\\in I} s_i\\, x_i\n",
    "\\\\[6pt]\n",
    "\\text{s.t.}\\quad\n",
    "& \\sum_{i\\in I} x_i \\;=\\; k\n",
    "\\\\[4pt]\n",
    "& x_i \\in \\{0,1\\}, \\qquad \\forall\\, i\\in I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The objective maximizes the total cosine similarity between the selected documents and the query.  \n",
    "The equality constraint enforces that exactly $k$ documents are chosen.  \n",
    "This formulation is **mathematically equivalent** to the heuristic of sorting all documents by their similarity scores $s_i$ and picking the top $k$ entries.  \n",
    "It thus provides a simple, interpretable baseline before introducing robust or multi-objective refinements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ae9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30245f01",
   "metadata": {},
   "source": [
    "# Enhancement Over Heuristic\n",
    "Motivated by recent findings (https://arxiv.org/abs/2502.09017) that **diversity improves RAG performance** (e.g., MMR/FPS-style selection), we extend the top-\\(k\\) similarity heuristic to a **goal-based and bi-objective model** that (i) promotes **diversity** via a constraint on the *average pairwise cosine* among selected documents and (ii) lets the solver **adapt the number of selected items** by adding a sparsity penalty (useful when some queries are simple while others are complex).\n",
    "\n",
    "\n",
    "**Sets & Index**\n",
    "- $I=\\{1,\\dots,n\\}$: index set of candidate documents (single query)\n",
    "\n",
    "**Parameters**\n",
    "- $q\\in\\mathbb{R}^d$: query embedding  \n",
    "- $\\mu_i\\in\\mathbb{R}^d$: embedding of document $i\\in I$  \n",
    "- $s_i := \\dfrac{q^\\top \\mu_i}{\\|q\\|_2\\,\\|\\mu_i\\|_2}$ (cosine similarity to the query; if embeddings are unit-normalized, $s_i=q^\\top \\mu_i$)  \n",
    "- $\\operatorname{cos}_{ij} := \\dfrac{\\mu_i^\\top \\mu_j}{\\|\\mu_i\\|_2\\,\\|\\mu_j\\|_2}$ (cosine similarity between documents $i$ and $j$)  \n",
    "- $\\rho_{\\text{div}}\\in[0,1]$: **diversity coverage ratio** (upper bound on the *average* pairwise cosine within the selected set)  \n",
    "- $\\lambda \\ge 0$: sparsity regularization weight (penalizes the number of selected documents; larger $\\lambda$ $\\Rightarrow$ fewer documents)  \n",
    "\n",
    "**Decision Variables**\n",
    "- $x_i\\in\\{0,1\\}$: equals 1 if document $i$ is selected; 0 otherwise  \n",
    "- $y_{ij}\\in[0,1]$: linearization variable representing co-selection ($y_{ij}\\approx x_i x_j$) for all $i<j$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x,\\,y}\\quad \n",
    "& \\sum_{i\\in I} s_i\\, x_i \\;-\\; \\lambda \\sum_{i\\in I} x_i\n",
    "\\\\[8pt]\n",
    "\\text{s.t.}\\quad\n",
    "& y_{ij} \\;\\le\\; x_i, \\qquad y_{ij} \\;\\le\\; x_j, \\qquad\n",
    "  y_{ij} \\;\\ge\\; x_i + x_j - 1, \n",
    "&& \\forall\\, i<j\n",
    "\\\\[6pt]\n",
    "& \\sum_{i<j} \\operatorname{cos}_{ij}\\, y_{ij} \\;\\le\\; \\rho_{\\text{div}} \\sum_{i<j} y_{ij},\n",
    "&& \\text{(average pairwise cosine $\\le \\rho_{\\text{div}}$)}\n",
    "\\\\[6pt]\n",
    "& x_i \\in \\{0,1\\}, \\quad y_{ij}\\in[0,1],\n",
    "&& \\forall\\, i<j,\\ \\forall\\, i\\in I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "**Interpretation.**  \n",
    "- **Objective.** The term $\\sum_i s_i x_i$ rewards query relevance (cosine to the query). The penalty $-\\lambda \\sum_i x_i$ **adapts the set size**: for simple queries (few strong matches), the model selects fewer items; for complex queries, it admits more items when their added relevance outweighs the penalty.  \n",
    "- **Diversity (goal-based) constraint.** $\\sum_{i<j} \\operatorname{cos}_{ij} y_{ij} \\le \\rho_{\\text{div}} \\sum_{i<j} y_{ij}$ enforces that the **average** pairwise cosine among the selected documents does not exceed $\\rho_{\\text{div}}$. Lower $\\rho_{\\text{div}}$ $\\Rightarrow$ stronger diversity (less redundancy).  \n",
    "- **Linearization.** The $y_{ij}$ constraints are the standard McCormick links for $x_i x_j$, keeping the model as a MILP.  \n",
    "\n",
    "This model is a **goal-based extension** of the top-\\(k\\) heuristic: rather than fixing $k$, it **learns** an appropriate set size via $\\lambda$, while **constraining redundancy** through an interpretable average-cosine cap $\\rho_{\\text{div}}$, an idea aligned with MMR/FPS principles cited in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0aea1",
   "metadata": {},
   "source": [
    "# A Robustness View\n",
    "We now note that the diversity-augmented model above **does not control for uncertainty in embeddings**. Because document embeddings are **direct outputs of deep models**, each embedding can be written as  \n",
    "$$\\tilde{\\mu}_i = \\mu_i + \\varepsilon_i,$$\n",
    "where $\\mu_i$ is the nominal output and $\\varepsilon_i$ captures estimation noise/variance. In practice, different encoders $f$ map raw text into **different representation spaces**, so cosine similarities with a fixed query can shift substantially across models. For simplicity and identifiability, we assume the **query embedding is fixed** (treated as constant) and **only document embeddings vary**; this isolates uncertainty to the retrieved content while avoiding compounding ambiguity from moving both the query and documents simultaneously.\n",
    "\n",
    "Below is the **robust (adversarial) formulation** that keeps the uncertainty set **generic**. It may be any **norm-ball** (e.g., $\\ell_2,\\ell_\\infty,\\ell_1$) or a **polyhedral** set (e.g., budgeted $k$-sparsity). We present the **max–min** problem as follows.\n",
    "\n",
    "**Sets & Index**  \n",
    "- $I=\\{1,\\dots,n\\}$: candidate documents (single query episode)  \n",
    "- $\\{(i,j): i<j,\\ i,j\\in I\\}$: unordered document pairs  \n",
    "\n",
    "**Parameters**  \n",
    "- $q\\in\\mathbb{R}^d$: fixed query embedding  \n",
    "- $\\mu_i\\in\\mathbb{R}^d$: nominal embedding of document $i$  \n",
    "- $\\mathcal{U}_i$: uncertainty set for document $i$, **user-chosen**, e.g.,  \n",
    "  - **norm-based:** $\\mathcal{U}_i=\\{\\tilde{\\mu}_i:\\ \\|\\tilde{\\mu}_i-\\mu_i\\|_{*}\\le \\rho_i\\}$  \n",
    "  - **polyhedral:** $\\mathcal{U}_i=\\{\\tilde{\\mu}_i:\\ A_i \\tilde{\\mu}_i \\le b_i\\}$ (incl. budgeted $k$-sparsity masks)  \n",
    "- $\\rho_{\\mathrm{div}}\\in[0,1]$: diversity coverage ratio (upper bound on average pairwise cosine)  \n",
    "- $\\lambda\\ge 0$: sparsity (size) penalty weight  \n",
    "\n",
    "**Similarity Definitions (with uncertain embeddings)**  \n",
    "- Query–doc cosine under uncertainty:  \n",
    "  $s_i(\\tilde{\\mu}_i) := \\dfrac{q^\\top \\tilde{\\mu}_i}{\\|q\\|_2\\,\\|\\tilde{\\mu}_i\\|_2}.$  \n",
    "- Doc–doc cosine under uncertainty:  \n",
    "  $\\mathrm{cos}_{ij}(\\tilde{\\mu}_i,\\tilde{\\mu}_j) := \\dfrac{\\tilde{\\mu}_i^\\top \\tilde{\\mu}_j}{\\|\\tilde{\\mu}_i\\|_2\\,\\|\\tilde{\\mu}_j\\|_2}.$  \n",
    "\n",
    "**Decision Variables**  \n",
    "- $x_i\\in\\{0,1\\}$: 1 if document $i$ is selected; 0 otherwise  \n",
    "- $y_{ij}\\in[0,1]$: co-selection linearization variable for pair $(i,j)$, approximating $x_i x_j$  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x,\\,y}\\ \\min_{\\{\\tilde{\\mu}_i\\in\\mathcal{U}_i\\}_{i\\in I}}\\quad \n",
    "& \\sum_{i\\in I} s_i(\\tilde{\\mu}_i)\\, x_i \\;-\\; \\lambda \\sum_{i\\in I} x_i\n",
    "\\\\[8pt]\n",
    "\\text{s.t.}\\quad\n",
    "& y_{ij} \\;\\le\\; x_i,\\quad y_{ij} \\;\\le\\; x_j,\\quad\n",
    "  y_{ij} \\;\\ge\\; x_i + x_j - 1, \n",
    "&& \\forall\\, i<j\n",
    "\\\\[6pt]\n",
    "& \\max_{\\tilde{\\mu}_i\\in\\mathcal{U}_i,\\ \\tilde{\\mu}_j\\in\\mathcal{U}_j}\n",
    "  \\sum_{i<j} \\mathrm{cos}_{ij}(\\tilde{\\mu}_i,\\tilde{\\mu}_j)\\, y_{ij}\n",
    "  \\;\\le\\; \\rho_{\\mathrm{div}} \\sum_{i<j} y_{ij},\n",
    "&& \\text{(goal-based diversity under worst case)}\n",
    "\\\\[6pt]\n",
    "& x_i \\in \\{0,1\\}, \\quad y_{ij}\\in[0,1],\n",
    "&& \\forall\\, i\\in I,\\ \\forall\\, i<j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "**Interpretation.**  \n",
    "- The **outer maximization** chooses a subset that is relevant (via query–doc cosine) while paying a **sparsity penalty** $\\lambda\\sum_i x_i$, letting the solver adapt set size to query complexity.  \n",
    "- The **inner minimization** is an **adversary** choosing $\\tilde{\\mu}_i\\in\\mathcal{U}_i$ to **depress relevance** and **inflate redundancy** subject to the chosen uncertainty model (norm-based or polyhedral).  \n",
    "- The **diversity goal** is imposed **robustly**: even under worst-case perturbations of embeddings, the **average pairwise cosine** of the selected set must not exceed $\\rho_{\\mathrm{div}}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c9ebdc",
   "metadata": {},
   "source": [
    "# Deterministic Tractable Reformulation\n",
    "We now provide **deterministic reformulations** for the diversity-aware, sparsity-penalized model under two uncertainty families:\n",
    "(1) a **norm-based** (dual-norm) set, and (2) a **polyhedral $k$-sparsity** set. \n",
    "To keep the model tractable and generic, we adopt the standard practice of **pre-normalizing embeddings** and using the **dot product** as the similarity surrogate; i.e., $s_i \\approx q^\\top \\mu_i$ and $\\mathrm{cos}_{ij}\\approx \\mu_i^\\top \\mu_j$. \n",
    "This avoids nonconvex denominators from cosine normalization while preserving ranking behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12baf557",
   "metadata": {},
   "source": [
    "### 1) Norm-based uncertainty set (dual-norm penalties, with geometry constant)\n",
    "\n",
    "**Uncertainty.** For each $i$,\n",
    "$$\n",
    "\\mathcal U_i \\;=\\; \\{\\ \\tilde\\mu_i=\\mu_i+\\Delta_i:\\ \\|\\Delta_i\\|\\le \\rho_i\\ \\},\n",
    "$$\n",
    "where $\\|\\cdot\\|$ is any chosen **primal norm** (e.g., $\\ell_2,\\ell_\\infty,\\ell_1$). Its **dual norm**, denoted $\\|\\cdot\\|_{*}$, is defined by $\\|z\\|_{*}=\\max_{\\|x\\|\\le 1} x^\\top z$.\n",
    "\n",
    "**Query–doc robustification.** Using support functions,\n",
    "$$\n",
    "\\min_{\\|\\Delta_i\\|\\le\\rho_i} q^\\top(\\mu_i+\\Delta_i)\n",
    "\\;=\\; q^\\top\\mu_i \\;-\\; \\rho_i\\,\\|q\\|_{*}.\n",
    "$$\n",
    "\n",
    "**Doc–doc robustification (pairwise redundancy) with geometry constant.**  \n",
    "We bound\n",
    "$$\n",
    "\\sup_{\\substack{\\|\\Delta_i\\|\\le \\rho_i\\\\ \\|\\Delta_j\\|\\le \\rho_j}}\n",
    "(\\mu_i+\\Delta_i)^\\top(\\mu_j+\\Delta_j).\n",
    "$$\n",
    "**Step 1 (expand):**\n",
    "$$\n",
    "(\\mu_i+\\Delta_i)^\\top(\\mu_j+\\Delta_j)\n",
    "=\\mu_i^\\top\\mu_j+\\mu_i^\\top\\Delta_j+\\mu_j^\\top\\Delta_i+\\Delta_i^\\top\\Delta_j.\n",
    "$$\n",
    "**Step 2 (Hölder on mixed terms):**\n",
    "$$\n",
    "\\mu_i^\\top\\Delta_j \\le \\|\\mu_i\\|_{*}\\,\\|\\Delta_j\\| \\le \\rho_j\\,\\|\\mu_i\\|_{*},\n",
    "\\qquad\n",
    "\\mu_j^\\top\\Delta_i \\le \\|\\mu_j\\|_{*}\\,\\|\\Delta_i\\| \\le \\rho_i\\,\\|\\mu_j\\|_{*}.\n",
    "$$\n",
    "**Step 3 (scale perturbation–perturbation term):** write $\\Delta_i=\\rho_i u$, $\\Delta_j=\\rho_j v$ with $\\|u\\|\\le 1$, $\\|v\\|\\le 1$. Then\n",
    "$$\n",
    "\\Delta_i^\\top\\Delta_j=\\rho_i\\rho_j\\,u^\\top v\n",
    "\\;\\le\\; \\rho_i\\rho_j\\,\\kappa(\\|\\cdot\\|),\n",
    "$$\n",
    "where the **geometry constant**\n",
    "$$\n",
    "\\kappa(\\|\\cdot\\|):=\\max_{\\|u\\|\\le 1,\\ \\|v\\|\\le 1} u^\\top v\n",
    "$$\n",
    "depends only on the unit ball of the chosen norm.\n",
    "\n",
    "**Deterministic reformulation (norm-based).**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x,\\,y}\\quad \n",
    "& \\sum_{i\\in I} \\big(q^\\top\\mu_i - \\rho_i\\|q\\|_{*}\\big)\\,x_i \\;-\\; \\lambda \\sum_{i\\in I} x_i \\\\[4pt]\n",
    "\\text{s.t.}\\quad\n",
    "& y_{ij}\\le x_i,\\ y_{ij}\\le x_j,\\ y_{ij}\\ge x_i+x_j-1,\\quad \\forall i<j \\\\[2pt]\n",
    "& \\sum_{i<j} \\big(\\mu_i^\\top\\mu_j + \\rho_i\\|\\mu_j\\|_{*} + \\rho_j\\|\\mu_i\\|_{*} + \\rho_i\\rho_j\\,\\kappa(\\|\\cdot\\|)\\big)\\, y_{ij}\n",
    "\\ \\le\\ \\rho_{\\mathrm{div}}\\sum_{i<j} y_{ij} \\\\[2pt]\n",
    "& x_i\\in\\{0,1\\},\\ y_{ij}\\in[0,1].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Interpretation (norm-based).**  \n",
    "- Each item’s **relevance** shrinks by a dual-norm term $\\rho_i\\|q\\|_{*}$ tied to the chosen primal norm.  \n",
    "- **Redundancy** is conservatively inflated by mixed dual-norm terms plus the **geometry factor** $\\kappa(\\|\\cdot\\|)$ that captures how well two unit perturbations can align within that norm’s unit ball.  \n",
    "- The model remains a **generic, norm-agnostic MILP**; plug in any norm, and penalties adapt via its dual and $\\kappa$.\n",
    "\n",
    "**$\\kappa(\\|\\cdot\\|)$ for norm choices considered in this work are**  \n",
    "- $\\kappa(\\ell_2)=1$ (round Euclidean ball; perfect alignment bounded by 1).  \n",
    "- $\\kappa(\\ell_\\infty)=d$ (unit cube; all $d$ coordinates can align to 1 simultaneously).  \n",
    "- $\\kappa(\\ell_1)=1$ (unit diamond; worst-case alignment achieved by concentrating mass on one coordinate).  \n",
    "\n",
    "$\\kappa(\\|\\cdot\\|)$ is a **shape factor** of the norm ball: the “pointier” or more axis-aligned the ball, the more two perturbations can co-align, increasing worst-case redundancy. The rounder the ball, the smaller the alignment potential and the less conservative the bound.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6cc279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75d432",
   "metadata": {},
   "source": [
    "### 2) Polyhedral $k$-Sparsity Uncertainty (Budgeted Coordinate Erasures)\n",
    "\n",
    "**Motivation.**  \n",
    "In dense embeddings, **not every coordinate matters** for *directional closeness*. Many dimensions contribute negligibly to the cosine with the query or even introduce noise.  \n",
    "The adversary can exploit this by **zeroing up to $k$ coordinates** that disproportionately determine similarity (either query–doc or doc–doc).  \n",
    "This models encoder instability, pruning, or quantization noise, where minor directions are dropped without major semantic loss.\n",
    "\n",
    "---\n",
    "\n",
    "### (a) Definition of the Set\n",
    "\n",
    "For each document $i$ with embedding $\\mu_i \\in \\mathbb{R}^d$, introduce coordinate masks $z_{is}\\in[0,1]$:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mu}_{is} = \\mu_{is}(1 - z_{is}), \\qquad\n",
    "\\sum_{s=1}^d z_{is} \\le k, \\quad 0 \\le z_{is} \\le 1.\n",
    "$$\n",
    "\n",
    "The **polyhedral uncertainty set** is:\n",
    "\n",
    "$$\n",
    "\\mathcal{U}_i^{(k)} = \n",
    "\\left\\{\n",
    "\\tilde{\\mu}_i \\in \\mathbb{R}^d :\n",
    "\\exists\\, z_i \\in [0,1]^d,\\ \n",
    "\\sum_s z_{is} \\le k,\\ \n",
    "\\tilde{\\mu}_{is} = \\mu_{is}(1 - z_{is})\\ \\forall s\n",
    "\\right\\}.\n",
    "$$\n",
    "\n",
    "This set represents **budgeted coordinate erasures**, i.e., at most $k$ dimensions may be partially or fully removed.\n",
    "\n",
    "\n",
    "### (b) Integrality and LP Equivalence\n",
    "\n",
    "**From original inner minimization**\n",
    "$$\n",
    "\\min_{\\tilde{\\mu}_i \\in \\mathcal U_i^{(k)}}\\ q^\\top \\tilde{\\mu}_i\n",
    "\\;=\\;\n",
    "\\min_{z_i}\\ \\sum_{s=1}^d q_s\\,\\mu_{is}(1-z_{is})\n",
    "\\quad\\text{s.t.}\\quad\n",
    "\\sum_{s=1}^d z_{is}\\le k,\\ \\ 0\\le z_{is}\\le 1.\n",
    "$$\n",
    "\n",
    "**Flip to equivalent maximization of the loss**\n",
    "$$\n",
    "\\min_{z_i}\\ \\big(q^\\top\\mu_i - \\sum_{s=1}^d q_s\\mu_{is}\\,z_{is}\\big)\n",
    "\\;=\\;\n",
    "q^\\top\\mu_i \\;-\\; \\max_{z_i}\\ \\sum_{s=1}^d q_s\\mu_{is}\\,z_{is}.\n",
    "$$\n",
    "\n",
    "**Keep only positive-impact coordinates**\n",
    "Define $a_{is}:=(q_s\\mu_{is})_{+}=\\max\\{q_s\\mu_{is},0\\}$. Then\n",
    "$$\n",
    "\\max_{z_i}\\ \\sum_{s=1}^d q_s\\mu_{is}\\,z_{is}\n",
    "\\;=\\;\n",
    "\\max_{z_i}\\ \\sum_{s=1}^d a_{is}\\,z_{is},\n",
    "$$\n",
    "because the optimizer will set $z_{is}=0$ whenever $q_s\\mu_{is}<0$.\n",
    "\n",
    "**Primal (inner) LP**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{maximize}_{z_i}\\quad\n",
    "& \\sum_{s=1}^d a_{is}\\, z_{is}\n",
    "\\\\\n",
    "\\text{subject to}\\quad\n",
    "& \\sum_{s=1}^d z_{is} \\le k\n",
    "\\\\\n",
    "& 0 \\le z_{is} \\le 1, \\qquad s=1,\\dots,d.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Originally, one might impose $z_{is}\\in\\{0,1\\}$ (exactly $k$ coordinates removed). However, relaxing this to $z_{is}\\in[0,1]$ **does not change the optimum**, because the inner problem $\\max_z \\sum_s a_{is} z_{is}$ with $\\sum_s z_{is}\\le k$, $0\\le z_{is}\\le1$ is a **unit-weight 0–1 knapsack**. Its constraint matrix is **totally unimodular**, so the LP relaxation has **integral extreme points**. Therefore, the LP and IP are equivalent in optimal value. This guarantees exactness of the dualization step below.\n",
    "\n",
    "\n",
    "### (c) Dual of the Query–Doc Robustification\n",
    "\n",
    "For each document $i$, define $a_{is} := (q_s \\mu_{is})_{+}$ (positive contribution).  \n",
    "The inner adversarial problem is:\n",
    "\n",
    "$$\n",
    "\\max_{z_i}\\ \\sum_s a_{is} z_{is}\n",
    "\\quad\n",
    "\\text{s.t. } \\sum_s z_{is}\\le k,\\ 0\\le z_{is}\\le1.\n",
    "$$\n",
    "\n",
    "Dualizing this LP introduces $\\pi_i \\ge 0$ and $\\alpha_{is}\\ge 0$:\n",
    "\n",
    "$$\n",
    "\\sum_s a_{is} z_{is}\n",
    "\\le\n",
    "k\\,\\pi_i + \\sum_s \\alpha_{is}, \\qquad\n",
    "\\alpha_{is} \\ge a_{is} - \\pi_i,\\ \\ \\alpha_{is}\\ge 0.\n",
    "$$\n",
    "\n",
    "Thus the **robust lower bound** on query–doc relevance is:\n",
    "\n",
    "$$\n",
    "q^\\top \\tilde{\\mu}_i\n",
    "\\ge\n",
    "q^\\top \\mu_i - \\big(k\\,\\pi_i + \\sum_s \\alpha_{is}\\big).\n",
    "$$\n",
    "\n",
    "\n",
    "### (d) Dual of the Doc–Doc Robustification\n",
    "\n",
    "For document pair $(i,j)$ (one-sided perturbation on $j$), define $b_{ijs} := (-\\mu_{is}\\mu_{js})_{+}$.  \n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mu_i^\\top \\tilde{\\mu}_j\n",
    "\\le\n",
    "\\mu_i^\\top \\mu_j + k\\,\\pi_{ij} + \\sum_s \\beta_{ijs},\n",
    "$$\n",
    "\n",
    "with dual variables $\\pi_{ij}\\ge 0,\\ \\beta_{ijs}\\ge 0$ satisfying:\n",
    "\n",
    "$$\n",
    "\\beta_{ijs} \\ge b_{ijs} - \\pi_{ij}, \\quad \\beta_{ijs}\\ge 0.\n",
    "$$\n",
    "\n",
    "For two-sided erasures, apply this symmetrically or define $u_{ijs}$ for combined effects.\n",
    "\n",
    "\n",
    "### (e) Deterministic Reformulation with Dual Constraints\n",
    "\n",
    "Plugging the dual objectives back into the outer problem gives a **single-level optimization**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x,\\,y,\\,\\pi,\\,\\alpha,\\,\\pi_{ij},\\,\\beta}\\quad \n",
    "& \\sum_{i\\in I} \\Big(q^\\top\\mu_i - k\\,\\pi_i - \\sum_{s=1}^d \\alpha_{is}\\Big)\\, x_i\n",
    "\\;-\\; \\lambda \\sum_{i\\in I} x_i \\\\[4pt]\n",
    "\\text{s.t.}\\quad\n",
    "& \\alpha_{is} \\ge (q_s\\mu_{is})_{+} - \\pi_i,\\quad \\alpha_{is}\\ge 0,\\ \\pi_i\\ge 0,\\quad \\forall i,s \\\\[2pt]\n",
    "& y_{ij}\\le x_i,\\ y_{ij}\\le x_j,\\ y_{ij}\\ge x_i+x_j-1,\\quad \\forall i<j \\\\[2pt]\n",
    "& \\sum_{i<j} \\Big(\\mu_i^\\top\\mu_j + k\\,\\pi_{ij} + \\sum_{s=1}^d \\beta_{ijs}\\Big)\\, y_{ij}\n",
    "\\le \\rho_{\\mathrm{div}}\\sum_{i<j} y_{ij} \\\\[2pt]\n",
    "& \\beta_{ijs} \\ge (-\\mu_{is}\\mu_{js})_{+} - \\pi_{ij},\\quad \\beta_{ijs}\\ge 0,\\ \\pi_{ij}\\ge 0,\\quad \\forall i<j,s \\\\[2pt]\n",
    "& x_i\\in\\{0,1\\},\\ y_{ij}\\in[0,1].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### (f) Linearization to Obtain a Pure MILP\n",
    "\n",
    "We introduce auxilary variables w, r, s to linearize the multiplicative relationship of decision variables in the above formulation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x,\\,y,\\,\\pi,\\,\\alpha,\\,\\pi_{ij},\\,\\beta,\\,r,\\,s,\\,w}\\quad \n",
    "& \\sum_{i\\in I} r_i \\;-\\; \\lambda \\sum_{i\\in I} x_i\n",
    "\\\\[6pt]\n",
    "\\text{s.t.}\\quad\n",
    "& r_i \\;\\le\\; q^\\top \\mu_i \\;-\\; k\\,\\pi_i \\;-\\; \\sum_{s=1}^d \\alpha_{is}, && \\forall i\n",
    "\\\\\n",
    "& \\alpha_{is} \\;\\ge\\; a_{is} - \\pi_i,\\ \\ \\alpha_{is}\\ge 0,\\ \\ \\pi_i\\ge 0, && \\forall i,\\ \\forall s\n",
    "\\\\\n",
    "& r_i \\;\\le\\; M_i\\, x_i,\\ \\ r_i\\ge 0, && \\forall i\n",
    "\\\\[6pt]\n",
    "& y_{ij} \\;\\le\\; x_i,\\ \\ y_{ij} \\;\\le\\; x_j,\\ \\ y_{ij} \\;\\ge\\; x_i + x_j - 1, && \\forall i<j\n",
    "\\\\[6pt]\n",
    "& s_{ij} \\;\\ge\\; \\mu_i^\\top \\mu_j \\;+\\; k\\,\\pi_{ij} \\;+\\; \\sum_{s=1}^d \\beta_{ijs}, && \\forall i<j\n",
    "\\\\\n",
    "& \\beta_{ijs} \\;\\ge\\; b_{ijs} - \\pi_{ij},\\ \\ \\beta_{ijs}\\ge 0,\\ \\ \\pi_{ij}\\ge 0,\\ \\ s_{ij}\\ge 0, && \\forall i<j,\\ \\forall s\n",
    "\\\\[6pt]\n",
    "& w_{ij} \\;\\le\\; s_{ij},\\ \\ w_{ij} \\;\\le\\; M_{ij}\\, y_{ij},\\ \\ w_{ij} \\;\\ge\\; s_{ij} - M_{ij}(1-y_{ij}),\\ \\ w_{ij}\\ge 0, && \\forall i<j\n",
    "\\\\\n",
    "& \\sum_{i<j} w_{ij} \\;\\le\\; \\rho_{\\mathrm{div}} \\sum_{i<j} y_{ij}\n",
    "\\\\[6pt]\n",
    "& x_i \\in \\{0,1\\},\\ \\ y_{ij}\\in[0,1], && \\forall i,\\ \\forall i<j\n",
    "\\\\[8pt]\n",
    "& \\text{where } a_{is} := (q_s \\mu_{is})_{+},\\ \\ b_{ijs} := (-\\mu_{is}\\mu_{js})_{+},\\ \\ M_i,\\ M_{ij}\\ \\text{are valid big-}M\\ \\text{bounds.}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### (g) Interpretation\n",
    "\n",
    "- The adversary can **erase up to $k$ coordinates** per document, targeting those with the largest contribution to similarity.  \n",
    "- This reflects the fact that **a few key coordinates dominate cosine similarity**, while most are redundant.  \n",
    "- The resulting model is a **fully linear MILP**, exact under LP duality because the inner problem’s LP relaxation is tight.  \n",
    "- Parameters:\n",
    "  - $k$: level of adversarial sparsification (robustness strength),  \n",
    "  - $\\rho_{\\mathrm{div}}$: desired diversity bound,  \n",
    "  - $\\lambda$: regularization on retrieval sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64022f73",
   "metadata": {},
   "source": [
    "# Cutting Plane?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
